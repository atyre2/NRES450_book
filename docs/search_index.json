[
["index.html", "Lecture Notes for Biology of Wildlife Populations Preface", " Lecture Notes for Biology of Wildlife Populations Andrew Tyre 2018-01-29 Preface I assume you have installed R and RStudio to run examples shown in each chapter. I originally set out to write this book because no existing population dynamics texts had the right mix of topics, or didn’t address management related questions, or were far too advanced for an undergraduate audience. My educational objectives for the course, and for the curriculum in which it is embedded, are as follows. Estimate the abundance of a species witin a specific geographical area, and critically evaluate the utility of abundance estimates. Evaluate the impacts of habitat change on future population abundance. Assess how changes in harvest regulations will affect population abundance. Assess the effect of competitors, predators and other species on a focal species. Predict the consequences of alternative management actions in a Structured Decision Making framework using simple population models. Identify the management decision(s) and the decision maker(s) relevant to a wildlife issue. Articulate the objectives of stakeholders along a means/ends continuum. Enumerate available management actions and assemble alternative strategies. Make tradeoffs between competing objectives to select a preferred management strategy in an SDM framework. I think objectives 2, 3 and 4 could be subsumed into objective 5, but I’ve left them broken out for the moment. Objectives 6, 7, 8 and 9 relate directly to using the PrOACT model of Structured Decision Making. Objective 7 is the same as Larkin’s proximate/ultimate distinction, I think. NRES 311 currently addresses completely or at least introduces 1-3 and 6-9; I’m not sure about 4. "],
["chap-fundamental.html", "Chapter 1 The fundamental law of population dynamics 1.1 The laws of nature 1.2 The nature of models of nature 1.3 Glossary", " Chapter 1 The fundamental law of population dynamics \\[\\begin{equation} \\Delta N = B - D + I - E \\tag{1.1} \\end{equation}\\] This simple equation governs all changes in single species populations. The change in the abundance of a population in a given area across an interval of time \\(\\Delta\\), \\(\\Delta N\\), is the sum of the number of births \\(B\\), the number of deaths \\(D\\), the number of immigrants, or individuals arriving in the area from outside, \\(I\\), and the number of emigrants, individuals leaving the area, \\(E\\). As the interval of time gets smaller, we can write the fundamental law as a rate \\[\\begin{equation} N&#39; = bN - dN + i - e \\tag{1.2} \\end{equation}\\] where we’ve replaced the number of births and deaths with the products of the population abundance, N, and a per capita rate of birth and death. We’ve left immigration and emigration as fixed rates. The apostrophe notation for N means “instantaneous rate of change”, that is, the rate when the time interval \\(\\Delta \\rightarrow 0\\). Where things get interesting is when one or more of the rate constants in (1.2) or amounts in (1.1) on the right hand side of these equations are not constant. In particular, when they depend on \\(N\\), or on the abundance of other species, the dynamics of populations get very interesting. 1.1 The laws of nature The title of this chapter describes equation (1.1) as a “law” – what do I mean by law? Is it something that was out there, waiting to be discovered by humans, independent of our existence and thought? Or was it created by our thinking of it, consistent with reality but not of reality? Joe Rosen, formerly professor of physics at Tel Aviv University and the University of Central Arkansas, spent an entire volume thinking hard about these issues (Rosen 2010). His categorization of reality and what we can know about it is useful and easy to follow, so I will use it here. He begins with the notion that there is an objective reality that exists independent of our existence. The primary reason for this observation is the simple fact that nature pushes back. Imagine a world where you can fly; wouldn’t it be marvelous! If the world were not objective, but merely a construct of our imagination (a view of reality known as solipsism), then you could create this world, and fly. Unfortunately nature pushes back, and you will fall to the ground. So objective reality constrains what we can do. The opposite of objective is subjective. Our inner thoughts and feelings are subjective, that is, they are known only to us as individuals. You might tell me what you are thinking or feeling, but I have no independent way of verifying that information. Beliefs about objective reality are similarly subjective, in that two people can have different beliefs about reality. However, it is possible for us to conduct reality checks on our beliefs about reality. If enough of us get together to check our beliefs, and over time, agree to a consensus belief that passes reality checks, then this is about as close to objective knowledge as we can get. Rosen calls this form of knowledge intersubjective; it is different from subjective belief by virtue of its broader consensus amongst many people, and yet not fully objective by virtue of the fact that it was formed from our subjective perceptions of reality. Intersubjective knowledge is socially constructed knowledge, but is not consistent with the the post-modernist position that all reality is socially constructed. Our socially constructed, intersubjective beliefs are constrained by objective reality – not everything is possible. Even if a diehard post-modernist could convince a group of a 1000 people that she could fly unaided, she would not be able to do so. In the field of wildlife management science, the goal is the production of “reliable knowledge” (Romesburg 1981) to use in making management decisions. It is not uncommon to see exhortations from leaders in the field to make science based decisions, presumably a call to use reliable, or intersubjective, knowledge to decide which course of action should be followed. Unfortunately, as we will see in many examples throughout this book, people do not make decisions like that. Our subjective beliefs about many things, from religion to justice, affect what we think should be done. Inevitably, the more people are affected by a decision or policy related to wildlife management, the more political (i.e. subjective) the decision or policy will become. So a law of population dynamics is well tested intersubjective knowledge, or reliable knowledge, that we can use to make predictions about the consequences of management actions. As you will see below, a law will also have assumptions that must be met in order for it to apply. 1.2 The nature of models of nature Almost all of the equations in this book, other than the fundamental equation, are models of nature. That is, they are deliberate simplifications of what is really going on out in nature. If our models were exact replicas of nature, then we would have as much trouble understanding the model as we have understanding nature! It is easy to get caught up in the thrill (OK, easy for people who build models) of adding ever more biological realism to a model. But this does not necessarily help us make decisions in the face of complexity. A consequence of this deliberate simplification is that all models are in fact, fictitious. This might seem surprising. After all, works of fiction are by definition false. Not true. How can something that is known to be false help us understand nature? This is a problem that so vexed early philosophers of science that they deliberately ignored it for the better part of a century. Fortunately for the science of population dynamics, this philosophical high-mindedness did not stop scientists from using models to understand nature. Statistician George Box put it this way: All models are wrong. Some models are useful. — Box (1976). The key is identifying which models are useful. In this book the utility of a model is measured by the extent to which the model allows us to forecast the future consequences of management decisions. 1.3 Glossary Solipsism A metaphysical viewpoint that asserts reality is only the result of our imaginations. References "],
["chap-sdm.html", "Chapter 2 Making decisions 2.1 Problem 2.2 Objectives 2.3 Alternatives 2.4 Consequences 2.5 Tradeoffs 2.6 Exercises", " Chapter 2 Making decisions In Chapter 1 I defined a useful model as one that helps a manager make a decision. This means that an understanding of the process of decision making is fundamental to the process of modeling population dynamics in an applied setting, at least. In this chapter I introduce the concept of structured decision making, a simple framework for making management decisions under risk and uncertainty. I start with a simple example of a decision: whether or not to provide supplemental food to a population of Hihi, an endangered bird in New Zealand (Figure 2.1). Managers at Kapiti Island want to provide nectar feeders to support recently translocated Hihi because they believe that the vegetation in the new location is not as rich in nectar sources as the island from which they came. They believe that additional food will support population growth in the translocated population by either improving adult survival or the number of offspring produced per pair. What they do not know is how much food to provide, or whether some other aspect of the environment in Kapiti Island will limit population growth despite increases in food availability. Concerns have also been expressed that Hihi will become dependent on feeders, and unable to utilize natural food sources. Figure 2.1: The hihi, or stichbird. Image by digitaltrails. http://www.flickr.com/photos/digitaltrails/87192080/ (CC BY-NC-SA 2.0) Although this is a relatively simple decision, it has all the features that make decisions complicated and difficult. There are multiple competing outcomes, multiple ways of reaching those outcomes, and uncertainty about which actions will lead to which outcomes. In the face of all this complexity, it is very human to avoid making a decision — which is itself a decision with outcomes — and focus on acquiring more information or attempting to pass responsibility for the decision onwards. Research throughout the 20th century has documented the many ways in which human cognition fails when faced with complex decisions. Psychologists have also studied ways in which to help humans make decisions in such circumstances. In this book I will introduce a simple 5 step approach to decision making (Smart Choices reference) that can help overcome the psychological traps of decision making; it has a simple easy to remember acronym PrOACT, reminding you to be proactive in decision making. The five steps are: Problem, Objectives, Actions, Consequences, Tradeoffs. 2.1 Problem The first step in making a decision is to ensure that you are in fact making the correct decision. What is the problem that has to be solved? Who has to solve that problem? Correctly identifying both the problem and the problem solver is critical to making a good decision. If the problem you think you’re trying to solve is in fact a decision at a higher level of organization, then no matter how good your decision making is you will not be able to solve the problem. Someone else is making the decision. You may be able to advise that person, but that is a very different role than directly making the decision yourself. What is the problem to be solved in the Hihi example? Is it identifying the direct effects of food supplementation on adult survival? What kind of monitoring program should be in place? How many translocated populations should receive food supplementation? All of these are legitimate questions. The key to identifying the right problem to solve is to focus on things that are under the control of the decision maker. If the manager of Kapiti Island is the decision maker, then deciding which translocated populations should receive food supplementation is beyond her scope — that will be decided by someone else. Similarly, a management problem will typically involve a choice among alternatives, rather than a resolution of an uncertainty. Identifying the direct effects of food supplementation on adult survival, while potentially useful information, does not involve a choice between alternatives by the decision maker. Choosing what to monitor in the park, and how, is a choice among alternatives – there are many things that could be measured. Similarly how many feeders and where to place them leads to choices among alternatives by the manager of Kapiti Island. Careful thought about who is making the decisions, and what decisions they have to make, is a critical first step in problem solving, and often the most neglected. The more effort devoted to this step the better, although it is also possible to get bogged down at this stage. Rather than attempting to solve all of the problems at once, it is useful to tackle individual decisions completely, and sequentially. Sometimes the solution to one problem turns out to depend on the solution to a different problem – the decisions are linked. Linked decisions are more complex than decisions that can stand alone, but breaking linked decisions into separate problems and tackling each independently makes it easier. In the Hihi example, the choice about what to monitor and how depends to a great extent on what choices are made about feeding stations. So first laying out the feeding station decision will provide a great deal of information about how to solve the monitoring problem. 2.2 Objectives Once the problem has been (tentatively) decided on, the next step is to determine how to measure success. The terms used for measuring success vary widely, and different texts will argue for critieria, metrics, goals, objectives, targets and many others besides. I will try to keep it simple. An objective is an attribute of the system being managed that has some value to the decision maker. In the Hihi example, the number of adult Hihi in Kapiti Island is a measurable attribute. The annual adult survival probability is another measurable attribute. The number of feeding stations in the park is a measurable quantity as well. How do you decide which and how many objectives are necessary? One approach to this problem is to categorize the measurable quantities into either means objectives or fundamental objectives. Distinguishing between these two is relatively easy. For each putative objective, you must ask yourself, “why does this matter?”. If the answer is simply, “because it does”, then the objective is likely a fundamental objective. In contrast, if the answer is “it helps achieve objective X”, then it is a means objective — a useful step, but not of interest in and of itself. Take a look at the quantity “number of feeding stations in Kapiti Island”. Does the manager care about this quantity? If there were no Hihi, would having 5 or 10 stations be better than having none? If yes, then the number of feeding stations would be fundamental — more feeding stations is better, regardless of what else is happening. However, this seems unlikely. The only reason for feeding stations is to help the Hihi population, so this is a means objective rather than a fundamental objective. In contrast, asking is 20 Hihi better than zero Hihi leads to a different conclusion – regardless of what else is happening in the park, 20 Hihi are better than none. The population size of Hihi on Kapiti Island seems like a fundamental objective, at least for this decision. The adult survival probability is an example of a more difficult objective. All other things equal, is a population with higher survival better than a population with lower survival? Imagine that there are 100 Hihi in the park, but you can have 100 birds with an annual survival of 0.85, or 100 birds with an annual survival of 0.9. Which is better? This is not so straightforward to sort out, and the answer may well depend on other vital rates of the population, and how they vary with population size. For example, what if the population of 100 with a low adult survival rate is only maintained by continued immigration from another population? The population in Kapiti Island is then a sink, and could be causing the larger metapopulation of Hihi to be lower than it otherwise would. This sort of complex interaction between potential objectives is not uncommon in fisheries and wildlife management problems. A more nuanced approach to the means vs. fundamental dichotomy is to construct an objectives hierarchy. Each potential objective is given a node, and then connections are made between nodes in order to identify causal relationships between them (see Figure 2.2). Figure 2.2: An objectives hierarchy for the hihi feeding problem. 2.3 Alternatives Once the objectives are described, the next step is to lay out all the alternatives, the different possible choices that can be made. This step is crucial, and requires creativity. It is all too easy at this stage to fall into the trap of only specifying alternative outcomes that are thought to be acceptable. The key is to lay out the broadest range of alternatives possible — even if you think some of them are impossible to achieve. The subsequent exercise of evaluating the consequences and tradeoffs will demonstrate which alternatives are impossible, but may also help to identify new alternatives that had not been previously considered. This is especially true for combinations of alternative actions. A good source for alternatives is to examine the lower end of the objectives hierarchy – those “means objectives” that are not ends in themselves, but lead to improvements in the fundamental objectives. In many instances these are attributes of the system that can be manipulated, not just measured. In the Hihi feeding station problem, the obvious place to start is with the number of feeding stations. This is something that is under the control of the park manager. Keeping a broad mind about the alternatives suggests we should consider everything from placing no feeding stations to placing many more than have been tried in the past. We should also consider alternatives about where to place the stations. Hihi management also involves placing nestboxes, so feeders should only be placed where there are nest boxes, but how many feeders per nestbox? Should we put feeders with all nestboxes? Do we need to maintain the feeders year round, or only during the breeding season? Suddenly we have many more things that we can manipulate than we expected! For the purposes of this example, let us define 5 alternatives as combinations of 0, 1 or 2 feeders per nest box, and maintaining feeders either year round or only in the breeding season. We end up with 5 alternatives, rather than 6, because 0 feeders per nest box doesn’t require choosing a duration of feeding. 2.4 Consequences Once we have our objectives and alternatives, the next step is to articulate the consequences of each alternative for each objective. One of our objectives is the number of Hihi, so this is the part where modeling population dynamics comes in. In essence, we want to predict what the population size will be in the future for each of our alternatives. This step is also where we find out how much we really know about the system we are trying to manage. Predicting consequences doesn’t have to be hard. For a first cut at a problem, it can simply be a statement of what relevant experts think will happen. Even ranking alternatives can be sufficient as a starting point, and that can usually be done with relatively little information. I started by assuming that no feeding stations would be the worst for population size and adult survival, and of course, have the lowest cost (Table 2.1). I also assumed that having 2 feeding stations per nest box would have the largest effect on population size, and be the most expensive. Notice that all options are tied for adult survival — this is because further consulting with experts (i.e. reading) revealed that supplementary feeding affects reproduction but not survival. That last observation led me to suppose that having 2 feeding stations only during the breeding season would have an equal effect on population size as maintaining the stations year round, but would of course be cheaper. By analogy then, having 1 feeding station all year will be equivalent to 1 feeding station for the breeding season, but both are less good than having 2 stations. Finally, I assumed that having 2 feeding stations for the breeding season would cost the same as having 1 station for the entire year, and having a single station just for the breeding season was the cheapest next to having no stations at all. Table 2.1: Prototype consequences table for the Hihi feeding problem. Alternative Cost # of Hihi Adult Survival 0 FS 1 1 1 1 FS all year 3 2 1 2 FS all year 4 3 1 1 FS breeding 2 2 1 2 FS breeding 3 3 1 2.5 Tradeoffs The final step in the process is to examine the consequence table to identify strategies or alternatives that perform the best. If you’re lucky, there’s one alternative that beats all others. Most of the time, you’ll not be so lucky, and there will be some alternatives that win on one objective, and others that win on a second objective. Somehow the different objectives must be “traded off” against one another. There are many ways of doing this, and for the most part these boil down to weighting the objectives in some fashion. But before getting to that level of complexity, it is always worthwhile to see if the decision problem can be simplified. The first tactic for simplification is to examine the objectives and see if any of them are uninformative. An uninformative objective is one that does not distinguish among the alternatives. If it isn’t distinguishing among the alternatives, then in effect it can be ignored. In the Hihi problem adult survival is uninformative (Table 2.1); it has the same value for all alternatives. So we can simply remove it from the consequences table, and proceed with a problem that is now much simpler — only 2 objectives to worry about instead of 3 (Table 2.2). Table 2.2: Consequences table for the Hihi feeding problem after simplifying by removing irrelevant objectives. Alternative Cost # of Hihi 0 FS 1 1 1 FS all year 3 2 2 FS all year 4 3 1 FS breeding 2 2 2 FS breeding 3 3 The second tactic for simplification is to look for strategies that are “completely dominated” by one or more other strategies. That is, the alternative performs equal or worse on every single objective. For example, take a look at the single feeding station for the entire year alternative. It costs more (3 vs 2) and has the same effect on population size (2 vs 2) as the single feeding station in the breeding season alternative. So, in this simple prototype of the decision problem, there would be no circumstances in which we would choose to run the feeding station for the entire year. If there were, then that would suggest that there is a fundamental objective that has not been captured, or perhaps our ranking of alternative performance is incorrect. In contrast, compare the no feeding stations alternative with the single station for the breeding season alternative. No feeding station is cheaper, but the single breeding season station does better for population size. In this case, there is no way to eliminate one of these alternatives — we will be forced to trade off the cost against the benefit to the population size. A similar argument allows us to eliminate the alterative with 2 feeding stations for the entire year — it does not perform better in any respect than 2 feeding stations just in the breeding season. We now have a much simpler problem to look at — just 3 alternatives and 2 objectives (Table 2.3). At this point we could try the 1st simplification tactic again — having eliminated some alternatives can leave some objectives irrelevant to the remaining set. However, in this case we’ve gone as far as simplification can get us. Now we need to do something more complex. It turns out that one of the flaws of ranking alternatives now comes back to bite us. Table 2.3: Consequences table for the Hihi feeding problem after simplifying by removing dominated alternatives. Alternative Cost # of Hihi 0 FS 1 1 1 FS breeding 2 2 2 FS breeding 3 3 Ranking destroys any information (or in this case, I never elicited that information) about how far apart each alternative is on that objective. For example, probably the biggest cost of maintaining the feeding stations is the personnel needed to visit each station. However, in this case, someone has to visit each nest box anyway to check for parasites. So as long as visits to feeding stations don’t need to happen more often than visits to nest boxes, the cost difference between the no feeding station alternative and the single station is not that great. In contrast, the increase to having 2 feeding stations could be larger, as now more material will have to be carried in rough terrain, and more travel between feeding stations will be required. Exactly how we resolve the tradeoffs between cost and population size benefits will depend a great deal on details like this. But now, instead of trying to worry about everything, we are very focused on the particular details that distinguish the remaining alternatives from each other. So let’s fill in the table with some values that have meaningful units on them. After some discussion, we decide that one feeding station doesn’t take extra time, but having 2 stations to visit will take an extra hour, on average. We’ll measure the population size relative to the change obtained by putting in one feeding station, so the 1 feeding station alternative gets a value of 1, and the value for 2 feeding stations will give us the proportional improvement for a second feeding station (Table 2.4). Now we revisit our simplification tactics, and we can see that the no feeding station alternative is completely dominated by the single feeding station alternative. We’re really getting somewhere now! The problem has reduced down to determining if an extra person-hour per week is worth a hypothesized 50% increase in population size. Table 2.4: Consequences table for the Hihi feeding problem after converting consequences to meaningful units. Alternative Cost [extra person-hours/week] # of Hihi [increase relative to 1 feeding station] 0 FS 0 0.0 1 FS breeding 0 1.0 2 FS breeding 1 1.5 I added a new word to the description in the last paragraph: hypothesized. This emphasizes something that we’ve so far ignored. To some extent, all of the rankings and quantities that we used so far are uncertain — we do not know them exactly. And in many cases they will in fact vary naturally — the amount of work required to maintain one or two feeding stations per nestbox probably varies by nestbox. The terrain is different, the stations are harder to travel between, the liquid in the feeding stations evaporates faster on warmer days, and so on. This kind of variation can make a big difference when making tradeoffs between objectives, because it introduces risk into the equation. It is possible that we will not obtain the benefits we seek, or that the costs will be higher. In the case of the improvement in population size, I’ve hypothesized that 2 feeding stations are better than one, but not twice as good. I’m not very certain about that however; it could be as low as 1 (no improvement at all) to as high as 2. There are many methods for proceeding in the face of uncertainty like this, but for the moment, I will stick with the point estimate. I feel that the extra benefit is not worth the extra cost. As long as the population is increasing, I am satisfied that we are meeting the overall goal, and therefore I choose the less expensive alternative. 2.6 Exercises See Table 2.5 for the consequences of a decision about how many Muskox to remove from Nunivak Island AK, identify all the alternatives that are completely dominated. How many objectives are redundant, which ones, and why? Table 2.5: Consequences table for Muskox removal problem. Alternative Cost [$] Range Condition Total Population, April Herd sex ratio [males:females] None 0.0 0.0 750 56:44 10 % of total 1.0 1.0 700 56:44 20 % of females 2.5 1.5 500 70:30 20 % of total 2.0 1.2 600 56:44 Read the problem description for wild horse management. Draw up a proposed objectives hierarchy for the managers to consider. Wild horses in the American Southwest Wild horses have roamed the west since their re-introduction to North America by the Spanish Conquistadors. Most of these horses occur on land managed by the Bureau of Land Management, and since 1971 the Wild Free-Roaming Horses and Burro’s Act has stipulated what BLM can and cannot do to manage this population. The BLM and other federal agencies are supposed to monitor horse numbers, and ensure that they “preserve and maintain a thriving natural ecological balance.” Although current management targets a maximum population of 23,622 horses across 179 management units, the current free-roaming population exceeds 33,000. Political pressure means that BLM is not able to cull horses, instead relying on capture and adoption to reduce wild populations. Unfortunately adoption demand is usually much less than the number of horses captured, and as a result there is a burgeoning captive herd of around 45,000 horses that must also be maintained. A few horses are sold, but most remain in captivity until they die of natural causes. Garrott and Oli (2013) worked out that the captive population would stabilize around 60,000 horses by 2027 representing a balance between natural deaths and new captures from the wild. Caring for these animals will cost in excess of $100 million dollars per year; the 2012 budget for the Wild Horse and Burro program was $74 million, of which 60% is used to care for captive horses. A National Research Council report (NRC 2013) on wild horses and burros concluded that if removals from the free-roaming population ceased, the population would grow unchecked until food and water became limiting. At that point the horses would be in poor health, and rangelands would be degraded affected native species and other public uses (grazing, hunting). Garrott, R.A. and M.K. Oli (2013) A critical crossroad for BLM’s Wild Horse Program. Science, 341:847-848. NRC (2013) Using Science to Improve the BLM Wild Horse and Burro Program: A Way Forward. National Academies Press, Washington, DC. "],
["chap-abundance.html", "Chapter 3 Estimating Abundance 3.1 Sampling error, accuracy and precision 3.2 Simple random sampling 3.3 How big a sample to take? 3.4 Areas of different sizes 3.5 Stratified Sampling Further Reading Exercises", " Chapter 3 Estimating Abundance The fundamental metric of population dynamics is the abundance of the population. Entire books, indeed lifetimes of research, have been devoted to this topic. Here I do not attempt a complete coverage of the topic, but merely introduce the most basic approach – counting organisms in a sample of areas of known size. We will learn how to calculate the uncertainty in an estimate of abundance, and how that uncertainty changes with effort devoted to sampling. This is an important concept for a manager of wildlife, because it is necessary to figure out how much information is good enough for the purpose at hand. More accurate information is certainly “better” in some abstract sense, but when that accuracy comes at a cost in resources, a tradeoff (in the very specific sense of chapter 2) must be made between the accuracy of the information and other objectives to which those resources could be bent. Why bother to estimate the size of the population? Why not simply count every individual animal, and obtain a census of the population? This was standard practice for large mammals in North America until the middle of the 20th century, and even later in Africa. With enough well trained personnel, it is indeed possible to obtain high levels of complete counts for populations, particularly in open areas like the African savannah. The primary reason for abandoning complete counts is cost. The time required to survey every piece of a landscape or region is tremendous, and time is money. In addition, for many species in many landscapes, the assumption of complete detectability, that every individual in a sample unit is counted, is not met. In that case, the attempt to census the population leads to an abundance that is biased (more on this later). We can both reduce costs and deal with bias of incomplete detectability by moving to statistical sampling procedures for estimating the total abundance of a population in a given area. 3.1 Sampling error, accuracy and precision When we want to measure \\(N\\), the abundance of a population in a particular area, we have two choices: census or sample. A census means that every individual is counted – no one is missed, no one is counted twice, the answer is \\(N\\), exactly. In that case, we know what \\(N\\) is, and there is no error to estimate. In this case, “error” has a very specific meaning. The error is the difference between the true \\(N\\) and the \\(N\\) that we have measured. In reality, the error is never zero. Even if a survey of abundance is called a census, it is virtually impossible to have an exact count of a real population. At best the error in the count is small and ignorable. A much better approach is to recognize the existence of error in our counts, and quantify it. From now on, to distinguish between the true population abundance, \\(N\\), and our estimate of the abundance, I will use a “hat” to indicate the estimated abundance, like this: \\(\\hat{N}\\). Error in an estimated quantity like the abundance has two components, the precision and the accuracy. Precision refers to the variation between repeated estimates of the same type – if we sampled the same population repeatedly, what is the average difference between repeated samples (Figure 3.1)? In contrast, accuracy refers to the distance between the average estimate and the truth. A good estimate will on average have no deviation from the true population abundance. Any individual estimate will be off by some unknown amount, but averaging repeated samples would converge to the true value. An estimate of a quantity that has a non-zero average difference between the estimate and the true value of the quantity is said to be biased. We want unbiased estimates of abundance. Figure 3.1: The classic depiction of the difference between accuracy and precision. The figure on the left shows high precision but low accuracy, while the figure on the right shows high accuracy and low precision. (Images from Wikimedia Commons, in the public domain) 3.2 Simple random sampling We’ll begin with the easiest possible estimate of abundance. We have some area, \\(A\\), which we call the sample frame. We divide the sample frame into \\(U\\) equal sized areas. We’ll call these subdivisions of the area sample units. The area of each sample unit is \\(a\\), and \\(Ua=A\\). We assume that we can count all the individuals in a sample unit without missing any individuals (perfect detection), and without counting individuals more than once (no double counting). We cannot count all \\(U\\) sample units. If we could, we would have a census, not a sample. Instead, we will select \\(u\\) sample units, where typically \\(u\\) is much smaller than \\(U\\). We will select the sample units at random. There are two ways to sample randomly from the set of all sample units. Give each sample unit a unique number, and place strips of paper with each number into a bag. In the first method, sampling with replacement, we draw a strip of paper, note the number, and then put the strip of paper back into the bag. This means that it is possible for a given sample unit to be counted more than once. In the second method, sampling without replacement, we draw a strip of paper, note the number, and then set that strip of paper aside. This means that each sample unit will only be sampled once. Both methods give the same result for the estimated abundance, but the precision of the samples will be different, especially if the total number of sample units U is small. Either way, we have now obtained a list of sample units in which to count our individuals. Having established which units to sample, we now obtain our counts which I’ll label \\(y_i\\) for the number of individuals counted in sample unit \\(i\\). From this dataset I can obtain the point estimate for the abundance in two steps. First, I calculate the average, or expected value, of the density for each unit \\(\\hat{D}\\). Second, I multiply this estimated average by the total number of units in the sample frame \\(U\\) to obtain the estimated abundance. The expected value of \\(\\hat{D}\\) is given by \\[\\begin{equation} \\hat{D}=\\frac{\\sum_{i=1}^u{y_i}}{\\sum_{i=1}^u{a_i}} =\\frac{\\sum_{i=1}^u{y_i}}{ua} =\\bar{y}\\frac{1}{a} \\tag{3.1} \\end{equation}\\] which has units of individuals per unit area. If you look back into your introductory statistics textbook, this is simply the arithmetic mean (\\(\\bar{y}\\)) multiplied by \\(1/a\\). This is the conversion from an average count per sample unit to an average density per unit area. The estimated abundance in the entire area is then \\[\\begin{equation} \\hat{N}=\\hat{D}A. \\tag{3.2} \\end{equation}\\] This estimate will be unbiased, or accurate, if our counts in each sample unit are made without errors. All individuals present are detected, no individuals are counted twice, and no other species are mistakenly identified as the species of interest. These are challenging conditions to meet, in most cases. The next step is to quantify the precision of our estimate. This is the step that differs between sampling with and without replacement. The simplest formula is for the case where sampling is done with replacement. This is the same formula for the standard error of a mean when sampling from an infinite population. The first step is to calculate the sample variance of the counts \\[\\begin{equation} s_y^2 = \\frac{\\sum_{i=1}^{u}{(y_i-\\hat{D}a_i)^2}} {u-1} \\tag{3.3} \\end{equation}\\] This is the variance of the distribution of the counts, not yet the precision of our sample mean. The sample variance is a property of the observed counts. In contrast, the precision of our sample mean is a property of a statistic, the average density. This statistic itself has a distribution, that is, it is a random variable whose value is not known precisely. Imagine taking a second, and then a third, sample of units and recalculating the average density. Each time you take a sample, the average density will be different. Imagine doing that infinitely many times, and you have the distribution of the average density. The best estimate of the expected value of the distribution of the average density is just the sample mean. But what about the variance of the distribution of the average density? The sample variance seems relevant; a more variable sample should lead to a less precise estimate of the sample mean. Intuitively, as more data are collected, the precision should improve the estimate of the mean gets better. So the variance of our average density should increase with the sample variance and decrease with the sample size, and it turns out that \\[\\begin{equation} s_{\\hat{D}}^2 = \\frac{1}{a^2}\\frac{s_y^2}{u} \\tag{3.4} \\end{equation}\\] is an unbiased estimate of the variance of the density. The square root of this variance \\(s_{\\hat{D}}\\) is given a special name, the standard error of the density. The term with \\(a^2\\) in the denominator simply ensures that the correct units are maintained. What if the sample was taken without replacement? In this case the finiteness of the sample has to be taken into account. As the number of samples taken, \\(u\\), increases towards the number of sample units available the sample becomes less of a sample and more of a census. In the extreme, sampling all of the units, the count is a census with no sampling variation at all. Thus the precision of our estimated mean should increase as the fraction of sample units counted increases. Equivalently, this means that the variance of the sample mean should decrease as the number of units counted rises towards the number available. The usual way to achieve this is to use the finite population correction in the formula for the variance of the sample mean \\[\\begin{equation} s_{\\hat{D}}^2 = \\frac{1}{a^2}\\frac{s_y^2}{u}\\left(1-\\frac{u}{U}\\right) \\tag{3.5} \\end{equation}\\] And when \\(u = U\\) this extra term equals zero, and the variance of the sample mean is zero. As a result of this correction factor, estimates from a sample without replacement are always more precise than a sample taken with replacement. The last step is to estimate the precision of the abundance \\(\\hat{N}\\). This quantity is a function of a random variable, \\(\\hat{D}\\), and a constant \\(A\\). There are several different ways to justify this, but for now just accept that the variance of the product of a random variable and a constant is just the product of the variance and the constant squared. In words that doesn’t sound so good, so here it is as a formula: \\[\\begin{equation} s_{\\hat{N}}^2= s_{\\hat{D}}^2 A^2 \\tag{3.6} \\end{equation}\\] which is pretty easy. With the estimated abundance and it’s precision in hand, there are a couple of other numbers worth calculating. The coefficient of variation is simply \\[\\begin{equation} CV_{\\hat{N}} =\\frac{s_{\\hat{N}}}{\\hat{N}} 100 \\tag{3.7} \\end{equation}\\] or the ratio between the standard error of abundance and abundance multiplied by 100. The CV is a useful relative measure of precision that can be readily compared between different estimates. Imagine you have 2 estimates, one of 100 individuals with a standard error of 10 and the second of a 1000 individuals with a standard error of 50. Which estimate is more precise? In an absolute sense the first estimate has a smaller standard error. However, in a relative sense, the second one is more precise with a CV of 5% compared to a CV of 10%. Implicit in the use of the CV is that a deviation of a given size is less important if the abundance is larger. When presenting estimated abundances graphically or in tables it is standard practice to convert the estimates of the precision into confidence limits on the estimate. A confidence limit shows the range of values that would contain the true mean a certain percentage of the time, typically 95%, if the entire estimation process (including calculating the confidence limits) were repeated many times (Figure 3.2). This is a challenging concept to grasp — however there are a couple of simple rules for interpreting these limits. First, you may not infer that the distribution of possible population sizes is indicated by the confidence limits. Unfortunately this is exactly what most biologists would like to do. Second, smaller confidence intervals are better. Third, if the confidence limits do not include some specific constant (like a target population size), then you can say that the estimate is statistically significantly different from the constant value. Figure 3.2: Simulated abundance estimates with 95% confidence intervals. The horizontal dashed line is the true abundance in a sample frame with 100 units. The dots represent 20 different samples of 10 units (with replacement) from the sample frame. 3.3 How big a sample to take? Figure 3.3: The standard error decreases as sample size increases. This assumes simple random sampling with replacement, and each count has a mean of 10 and a variance of 5 (green line), 10 (black line), or 20 (red line). Should we care how many units we sample? Each additional unit increases the cost of the effort - at a minimum it takes time, and time is usually money. So why not use the smallest number of units possible? We can calculate a variance with a sample size of two, so why not use only two? As always in life, there is a tradeoff to be made here. Fewer sample units are cheaper, but the resulting estimate is less precise. Unfortunately, there is no “rule of thumb” that works for all samples. The greater the sample size, the more precise the estimate is all one can say. The response is also not linear – when sample sizes are small the increase in precision with each additional sample is greater than when sample sizes are large (Figure 3.3). The exact height of this curve depends on the variance of the counts in each sample unit, but the shape of the curve will always be a reciprocal function of the sample size. The curve is higher (worse precision for a given sample size) when the variance of the counts is higher for the same average count (red line in Figure 3.3). The curve is lower (better precision for a given sample size) when the variance is smaller (green line in Figure 3.3). So how do we decide on the number of samples to take? We need to have some information about the mean and variance of the samples we are going to take, and a goal for the precision of our estimate. There are rules of thumb for the precision required, but they are nearly impossible to achieve. For rough monitoring a coefficient of variation of 20% is considered acceptable, while for research purposes 5% is the goal. The expected mean and variance of the samples comes either from previous surveys of the same species using the same methods, or expert opinion (also known as a SWAG for Scientific Wild Assed Guess). With this information it is possible to construct the curve in Figure 3.3, and then read off the sample size needed to achieve the goal. What typically happens is that the budget available sets the precision that can be achieved. It is important to note that the variance of the counts is not a function of the sample size. The estimated sample variance will change as more samples are taken, but does not change systematically. Rather, the sample variance converges towards the true variance of the counts, which is some positive number (Figure 3.4). At small sample sizes the variance of the sample variance is also higher. Figure 3.4: The variance of the counts does not systematically decrease as the sample size increases. The dashed line is the true variance for each sample 3.4 Areas of different sizes In the previous section the sample units were equal in size, so that the total area \\(A = Ua\\). In many instances the sample units may not be equal in size, and then a slightly different approach is needed for calculating the precision of the total abundance \\(N\\). The first issue is to choose the sample units randomly with respect to their size. The second issue is how to deal with differences in size when calculating the standard errors of density and hence abundance. One option for choosing sample units that differ in size is to ignore the differences, and choose sample units with equal probabilities. This leads to a ratio estimate of the abundance. The estimate of the density is \\[\\begin{equation} \\hat{D} = \\frac{\\sum_{i=1}^u{y_i}}{\\sum_{i=1}^u{a_i}} \\tag{3.8} \\end{equation}\\] which leads to an abundance estimate given by \\[\\begin{equation} \\hat{N} = \\hat{D} A. \\end{equation}\\] This is identical to the estimator used when the sample units have identical areas. The formula for the sample variance of the \\(y_i\\) is different because each count is weighted in the variance differently. \\[\\begin{equation} s_y^2 = \\frac{\\sum_{i=1}^u{y_i^2}+\\hat{D}^2\\sum_{i=1}^u{a_i^2}-2\\hat{D}\\sum_{i=1}^u{a_iy_i}}{\\left(u-1\\right)}. \\tag{3.9} \\end{equation}\\] With the sample variance for the counts calculated, we follow the same procedure to get the the variance of \\(\\hat{D}\\) \\[\\begin{equation} s_{\\hat{D}}^2 = \\left(\\frac{u}{\\sum_{i=1}^u{a_i}}\\right)^2\\frac{s_y^2}{u} \\tag{3.10} \\end{equation}\\] The finite sample size correction for sampling without replacement is similar - now instead of the fraction of sample units we use the fraction of the area: \\[\\begin{equation} s_{\\hat{D},SWOR}^2 = s_{\\hat{D}}^2\\left(1-\\frac{\\sum_{i=1}^u{a_i}}{A}\\right). \\tag{3.11} \\end{equation}\\] Finally, the conversion to the \\(s_{\\hat{N}}^2\\) is always the same, simply multiplying the variance by the square of total area, \\(A\\). \\[ s_{\\hat{N}}^2 = s_{\\hat{D}}^2 A^2 \\] The second alternative is to select sample units with a probability proportional to their size; this leads to a probability-proportional-to-size or PPS estimate. For example, if our sample units are defined in a GIS layer with different size polygons, and we select the units to be sampled by throwing random points onto the map, then each unit will be selected with a probability proportional to it’s size. In addition, it will also be sampling with replacement, and this is the only type of sampling that works for this estimator. 3.5 Stratified Sampling So far, all sampling has been carried out as simple random sampling, either with or without replacement. It turns out that this may not give the most accurate estimate of abundance. The implicit assumption is that all sample units have the same statistical parameters, that is, the same mean and variance of counts. If this is not true, if there are differences between sample units, then simple random sampling does not yield the most accurate estimate of abundance. How might such differences arise? The most obvious differences are ecological - not all areas are equally good habitat for a species. We expect to find more individuals in areas that are better habitat, and all else being equal, more individuals also means an increase in the variance of our counts. This kind of heterogeneity is the rule, rather than the exception, and therefore our simple random sampling is usually not the best estimator. In the presence of heterogeneity, the best estimate of abundance requires stratified random sampling, where the probability of selecting a sample unit is made proportional to the variance of the counts in that unit. In this way, more data are collected where the variances are greater, and the resulting estimate is more accurate. You might already be able to see the catch - how do we know which units have greater variance before we collect any data? As always, nothing comes for free! If we know what the variances are, then everything is easy. The sample frame is divided into strata in such a way that the variances in counts are equal within strata (or at least more equal within than between strata). There is a tradeoff to be made here – more strata means the heterogeneity within a stratum is smaller, but eventually the added complexity becomes self defeating. Strata that are larger or have higher variance receive a larger proportion of the sample units \\[\\begin{equation} u_h = u \\frac{U_{h} SE\\left(\\hat{N_h}\\right)} {\\sum_{i=1}^{H}{U_{h} SE\\left(\\hat{N_h}\\right)}} \\tag{3.12} \\end{equation}\\] where \\(u\\) is the total sampling effort to be allocated, and \\(U_h\\) is the total number of sample units available in stratum \\(h\\). This allocation, known as the Neyman allocation, is optimal, in the sense that the variance of \\(\\hat{N}\\) is as small as possible. If the cost of sampling each stratum is identical, then this will also yield the most precise estimate for a given cost. However, if sampling costs differ between strata, then we also want to change the proportion with the cost of each stratum in order to do the best possible for a given amount of money. What if we don’t know what the variances are? Typically we still have some notion of where we expect to find more or less of the species in question. Ideally, we already have the landscape categorized in some fashion, say into ecoregions, and we know that some ecoregional types are better than others. Or we know that certain land uses are more conducive to the species. For example, if you want to estimate the abundance of a prairie dependent species like the grasshopper sparrow, you would expect fewer of them in riparian forests than in perennial grasslands. In addition, the variance of a count typically increases with the abundance (see Taylor’s Law). Thus for stratification to work, it is sufficient to have an idea of the relative abundance, and hence relative variance in counts. Once we’ve divided the sample frame into \\(H\\) strata and decided how many units to sample in each stratum, we then simply calculate the abundance for each stratum \\(h\\) using the formulas given previously, and add them together \\[\\begin{equation} \\hat{N} = \\sum_{i=1}^{H}{\\hat{N_h}}. \\tag{3.13} \\end{equation}\\] The \\(\\hat{N_h}\\) for each stratum are calculated in exactly the same way as the abundance estimates for simple random sampling. Similarly, we can get the variance of the abundance as the sum of the variances of each stratum estimate. \\[\\begin{equation} var\\left(\\hat{N}\\right) = \\sum_{i=1}^{H}{var\\left(\\hat{N_h}\\right)}. \\tag{3.14} \\end{equation}\\] Imagine that you want to estimate the population of white tailed deer (Odocoileus virginianus) on a property along the Platte River in Nebraska. The property has two kind of habitats, riparian forest and upland pasture. You know that deer prefer forests, and so you expect deer to be four times as abundant in the forest as they are in the pasture. You’re planning drive counts on 10 ha blocks, and there are 100 ha of each type of habitat, so \\(U_{forest} = U_{pasture} = 10\\). If we can afford to carry out \\(u_{total} = 6\\) drive counts, how many should we do in the forest vs. the pasture? \\[\\begin{equation} u_{forest} = u_{total} \\frac{U_{forest} SE_{forest}} {U_{forest} SE_{forest} + U_{pasture} SE_{pasture}} \\ = 6 \\frac{10\\cdot \\sqrt{4}} {10 \\cdot \\sqrt{4} + 10 \\cdot \\sqrt{1}} \\ = 6 \\frac{20} {20 + 10} \\ = 4 \\end{equation}\\] In that case we should do 4 counts in the forest and 2 in the pasture. Note that we use the square root of the relative abundances, because the variance is proportional to abundance, but the allocation equation uses the standard errors. Even though the relative areas of the two habitats are identical, we do more counts in the forest because we expect the variance of the counts to be higher there. Taylor’s Law One of the most repeatable patterns in ecology is the relationship between the mean abundance and the variance of sample counts Ecologist Richard Southwood named this relationship “Taylor’s law” after his colleague L.R. Taylor, although it was known from agricultural data long before Taylor presented it in a 1961 Nature paper. In it’s linear form Taylor’s law is \\[ log\\left(s_y^2\\right) = a + b \\cdot log\\left(D\\right)\\,. \\] It is possible to use simple linear regression to estimate the constants \\(a\\) and \\(b\\) if you have estimates of the density from areas with a range of densities. The constant \\(b\\) can be shown to be equal to 1 when the organisms are distributed at random. Values of \\(b\\) less than one are indications of underdispersion, or even spacing between individuals, while values of \\(b &gt; 1\\) indicate aggregation between individuals. Thus, if the variance of the counts is greater than the mean of the counts, we say the population is “overdispersed”, or equivalently, that individuals are clumped together more than we expect from a random distribution. Further Reading Thompson, William, et al.(1998) Monitoring Vertebrate Populations. Academic Press. There’s a good cheat sheet for sample means and variances at Exercises Table 3.1: Stratified random sample of the Nelchina Caribou herd in Alaska by Sniff and Skoog (1964). Units are 4 square miles. Stratum Stratum Size, \\(U_h\\) Sample Size, \\(u_h\\) Mean # caribou per sample unit Variance of counts \\(S_y^2\\) \\(N_h\\) \\(var(N_h)\\) A 400 98 24.1 5575 B 30 10 25.6 4064 NA NA C 61 37 267.6 347556 NA NA D 18 6 179 22798 NA NA E 70 39 293.7 123578 NA NA F 120 21 33.2 9795 NA NA Total 699 211 NA NA NA NA Calculate the estimated abundance and variance of the entire Nelchina Caribou Herd (Table 3.1). Given the same total sample size, what is the Neyman allocation of sample units to strata in the Caribou example? "],
["chap-exponential.html", "Chapter 4 Exponential growth and decay 4.1 Deaths 4.2 Births 4.3 Turchin’s law of population inertia 4.4 Estimating \\(r\\) 4.5 Glossary 4.6 Exercises", " Chapter 4 Exponential growth and decay I want to revisit the fundamental law, and consider what happens when the rate constants are fixed. That is, the per capita rates of births, deaths, immigration and emigration are not changing with time. \\[\\begin{equation} N&#39; = bN - dN + i - e \\tag{4.1} \\end{equation}\\] 4.1 Deaths Of all the biological processes in the fundamental equation, the simplest one to get a handle on is \\(D\\), the number of deaths in a period of time. A great deal of fisheries and wildlife management involves directly manipulating \\(D\\) through harvest or culling, or indirectly manipulating \\(D\\) through habitat management. Consider the problem of stocking a lake with walleye fingerlings. From past research at this lake, we know that the instantaneous mortality rate is \\(d = 0.01 week^{-1}\\). If a hatchery manager wants to stock 10,000 fingerlings, how many will survive one year? We have three pieces of information – the rate of mortality, the initial number of fish, and the period of time. In continuous time, assuming that there are no births, emigration or immigration \\[\\begin{equation*} N&#39; = - dN \\end{equation*}\\] This is an equation for the rate of change in a variable where the variable itself (\\(N\\)) is multiplied by a constant. This is the equation for exponential decay. The solution to that equation is \\[\\begin{equation} N_t = N_0 e^{-dt} \\tag{4.2} \\end{equation}\\] We can substitute in the values for our problem, where N0 = 10000 and t = 52 weeks, which gives us \\[\\begin{equation*} N_{52} = 10000 e^{-(0.01)(52)} = 5945 fingerlings \\end{equation*}\\] In nature, the result will almost certainly not be exactly 5945 fingerlings. We do not know \\(d\\) exactly, \\(d\\) might not be constant over the course of the year, and indeed, the process of mortality itself is not deterministic. We cannot know exactly how many fingerlings will encounter a channel catfish during the year; only that some will with catastrophic results for the fingerlings! The number 5945 is an expected value, in the sense of Chapter 3. Instead of an instantaneous mortality rate, we might have a discrete time rate estimated from a mark-recapture program. In that case we would have a per capita mortality rate \\(d_{\\Delta t}=e^{-d \\Delta t}\\) Note that the discrete time rate depends on the interval over which it is estimated. 4.2 Births The second term in the fundamental equation that we must have is the number of births in a single unit of time, \\(B\\). There is little that can be done to manage \\(B\\) directly; most wildlife management efforts can at best manipulate \\(B\\) indirectly by altering the habitat. There are instances where preventing births through contraception is a useful management strategy for over abundant species, but it is often more expensive than manipulating \\(D\\). In the continuous time form, the rate of change in N from births is \\[\\begin{equation*} N&#39; = bN \\end{equation*}\\] where b is the per capita rate of births per unit time. The population size after some interval of time \\(t\\), \\(N_t\\) will be given by \\[\\begin{equation} N_t = N_0 e^{bt} \\tag{4.3} \\end{equation}\\] There are not very many species where births occur continuously (put in a box describing one of each type). Equation (4.3) is an approximation of what usually occurs with wildlife species, especially wildlife in temperate and polar systems. There are many circumstances in which this approximation is acceptable, but we must never forget that it is an approximation, not the objective reality. In Chapter 7 we will examine alternative approximations to the birth process that more closely match the biological reality. 4.3 Turchin’s law of population inertia With births and deaths, we can proceed to construct the simplest possible model of population dynamics. In order to do so we must make an assumption about the remaining two pieces of the fundamental equation, \\(I\\) and \\(E\\), the number of individuals moving into and out of the population. We have a couple of choices here, but let us start with the simplest assumption: \\(I = E = 0\\). That is, we will assume that no individuals move into or out of the population. The population is closed to emigration and immigration. With this assumption, the fundamental equation in continuous time reduces to \\[\\begin{equation} N&#39; = bN - dN =(b-d)N = rN \\tag{4.4} \\end{equation}\\] where we’ve defined a new constant \\(r\\), the difference between the per capita birth and death rates. This constant \\(r\\) is so important in population dynamics that it has a special name – the intrinsic rate of population growth. Notice that equation (4.4) looks a lot like equation (4.2) and (4.3). It is the equation for exponential growth or decay, and we know the solution for that: \\[\\begin{equation} N_t = N_0 e^{rt} \\tag{4.5} \\end{equation}\\] This is the equation for exponential population growth, and it is the simplest possible population model. When \\(r &gt; 0\\), the population will increase, and when \\(r &lt; 0\\) the population will decrease (Figure 4.1). Looking back at eq. (4.4), we can see that \\(r &gt; 0\\) when the per capita birth rate is greater than the per capita death rate. That makes sense – in order for the population to increase the number of births must exceed the number of deaths. Conversely, \\(r &lt; 0\\) when the per capita birth rate is less than the per capita death rate. The most important thing to recognize here is that the difference between a population growing or decaying is determined by the relative magnitudes of b and d. Measuring either one alone does not tell us whether the population will increase or decrease. What happens if \\(r\\) is exactly equal to zero? The population will neither increase nor decrease, but remain at exactly the same level. Figure 4.1: Population growth over time r is greater than or less than zero. The right hand panel shows the same curves on a log scale. Turchin’s law of population inertia states: in the absence of other forces, a population will continue to grow (or decline) exponentially (Turchin 2003). Equation (4.5) tells us the rate at which the population will grow or decline. The important part of the law is the first phrase, “in the absence of other forces”. In particular, the birth and death rates are independent of the population size (Figure 4.2). That is, assuming that the per capita birth and death rates remain constant, unaffected by other processes, then the law of population inertia holds. Much of the remainder of this book focuses on determining when the law of population inertia does not hold. Figure 4.2: Per capita birth and death rates are independent of population size in exponential growth (A) or decay (B). There is one additional property of the exponential model of population dynamics that makes it useful as a touchstone for analyzing changes in numbers. If we take the logarithm of both sides of Equation (4.5), we obtain \\[\\begin{equation*} ln(N_t) = ln(N_0) + rt \\end{equation*}\\] which has the same form as the equation of a straight line. The y-intercept of the graph is the logarithm of the population size when \\(t=0\\). Plotting the logarithm of \\(N_t\\) against time will then have a straight line with a slope of \\(r\\). This is a very easy way to check if a particular time series is well approximated by the exponential population model. 4.4 Estimating \\(r\\) If this constant \\(r\\) is so important, how do we figure out what value to use? Later we will build up an estimate of \\(r\\) from detailed life history information. It can also be done in a “back of the envelope” way as long as we have two estimates of population size at different times. We start with equation (4.5), and rearrange it to solve for \\(r\\) \\[\\begin{equation} \\begin{split} \\frac{N_t}{N_0} &amp; = e^{rt} \\\\ ln\\left(\\frac{N_t}{N_0}\\right) &amp; = rt \\\\ r &amp; = ln\\left(\\frac{N_t}{N_0}\\right) / t \\end{split} \\end{equation}\\] So our first estimate of \\(r\\) is the log of the ratio of population estimates, divided by the number of time periods between the two estimates. This estimate assumes that the law of population inertia held during the intervening \\(t\\) time periods. As an example, let us carry out these calculations for the herd of Muskox (Ovibus moschatus) on Nunivak Island, Alaska. Muskox were extirpated from the Arctic slope of Alaska in the late 19th century, and efforts to reintroduce them from populations in Greenland began in the 1930’s. In 1947 David Spencer of the Bureau of Sport Fisheries and Wildlife in Alaska began an annual survey of the Nunivak Island population (Spencer and Lensink 1970). He recorded a total of 49 animals in 1947, and in 1965 he counted 514 animals total. Assuming that the law of population inertia held during those 18 years, we find \\[\\begin{equation} r = ln\\left(\\frac{N_t}{N_0}\\right) / t = ln\\left(\\frac{514}{49}\\right) / 18 = \\frac{ln(10.49)}{18} = 0.13 \\, years^{-1} \\end{equation}\\] How should we interpret that estimate? The change in a population over 1 year is \\(Ne^r\\), so the factor \\(e^r\\) is the percentage change in population size. In this case, \\(e^r = 1.138\\), or about a 14 % change per year. A useful approximation to remember is that when \\(r\\) is close to 1, then \\(e^{r} \\approx 1+r\\), so you can think of \\(r\\) as a percentage change in population size per year as long as it isn’t too large. In the musk-ox case the error in this approximation is 1 percentage point. With this estimate of \\(r\\) in hand, there are several things we can do. For example, what if we want to predict the number of animals on the island after another 3 years, assuming population growth holds at the same rate? \\[\\begin{equation} N_3 = 514 e^{(0.13)(3)} = 514(1.48) = 759\\,muskox \\end{equation}\\] This is quite useful as a benchmark for management. If we go back in 3 years and there are a lot fewer, or a lot more muskox than we expected, then we would have to conclude that “all other forces” were not in fact zero, and that would start us on a useful search for what might have changed. In fact, in 1968 there were 714 animals. Is that a lot fewer than expected? In order to answer that question, we need to address an important issue – indeterminism – which we will begin in the next chapter. Another easy calculation to do is finding the doubling time for a population. This is the number of years for the population to double in size. The key here is to recognize that a population has doubled when \\(N_t = 2N_0\\). We substitute \\(2N_0\\) into eq. (4.5) and solve for \\(t\\) \\[\\begin{equation} \\begin{split} \\frac{2N_0}{N_0} &amp; = e^{rt} \\\\ ln\\left(2\\right) &amp; = rt \\\\ t &amp; = \\frac{ln\\left(2\\right)}{r}\\,. \\end{split} \\tag{4.6} \\end{equation}\\] For the musk ox we get \\(t = ln(2)/0.13 = 5.3 \\, years\\) for the population to double in size. This calculation is very handy for guiding management expectations. If we desire a population to be 10% larger, we can calculate how many years that will take by replacing the constant 2 in eq. (4.6) with 1.1. 4.5 Glossary Instantaneous mortality rate the rate at which mortality events occur when the interval of time is allowed to shrink towards zero. Deterministic processes have only a single outcome given complete knowledge of the state of the system. Expected value of a random variable is the mean value, or the 1st moment of the distribution. 4.6 Exercises An index of the Bald Eagle population in Illinois (Havera and Kruse 1988) grew from 188 individuals in 1970 to 569 individuals in 1987. Assuming a closed population with constant birth and death rates through this period, what was the annual population growth rate? For the same population of Bald Eagles in Illinois, how many eagles would you expect in 1984? If the actual population count in 1984 was 930, what could you conclude about this population? In how many years would you expect the Bald Eagle population to double in size after 1987? Wildlife managers at an east coast wildlife refuge want their population of Piping Plovers to increase by 20% in 5 years. Their estimated value of \\(r = 0.01\\) without changing any management strategies. Can they achieve their objective? What value of \\(r\\) would allow them to achieve their objective? References "],
["chap-regulation.html", "Chapter 5 Population Regulation and Stochasticity 5.1 Resource Limitation 5.2 Density Dependent or Independent? 5.3 Stochasticity 5.4 Trends over time 5.5 Glossary", " Chapter 5 Population Regulation and Stochasticity Although Turchin’s law is often observed for short periods of time, eventually it always breaks down. Why? Under what conditions does exponential growth (or decay) come to a halt, and can we predict those conditions in advance? Pheasants on the Island 8 pheasants were released on Protection Island in 1937 (Einarsen 1945). The pheasants were counted twice a year until the entire population was eaten in 1943 when troops were posted to the island (fact check this – not mentioned in Einarsen 1945)! There were 30 pheasants in 1938, so using equations from Chapter 4 we can work out the annual rate of growth assuming the population grows exponentially: \\[ r=ln\\left(\\frac{N_t}{N_0}\\right)/t = ln\\left(\\frac{30}{8}\\right)/1 = 1.32 \\] which is an extremely high rate of growth. The line in Figure 5.1 shows the projected population size assuming this growth rate continued through 1943. Clearly, from 1941 on the population was growing at less than the exponential rate. Why did the growth rate slow down? Figure 5.1: Total population size of pheasants on Einarsen Island. Filled circles are the spring counts, and open circles are the fall counts. The solid line is exponential growth with \\(r = 1.32\\). The panel on the right is on a logarithmic scale. 5.1 Resource Limitation Although we’ll never know for sure, it seems likely that the Protection Island Pheasants stopped growing so fast because they were lacking some resource needed for growth, survival, or reproduction. Competition for resources, either intra-specific competition with other members of the same species, or inter-specific competition with members of a different species, can reduce the resources available to each individual. If the per capita rates of any of the processes in the fundamental equation (4.1) are affected by the amount of resources available to an individual, then this competition can reduce the population growth rate. Consider a simple example. There is a resource that is critical to reproduction, so the per capita birth rate is proportional to the amount of resource per individual. Individuals of the population compete with each other in scramble fashion, so the amount each individual obtains is a simple fraction of the total available. More individuals in the population equal less resources for each. This resource limitation leads to a reduction in the per capita rate of reproduction \\[\\begin{equation} b\\left(N\\right)=\\frac{a}{N} \\tag{5.1} \\end{equation}\\] where \\(a\\) is a parameter describing how the per capita birth rate changes with population size. The parameter \\(a\\) is readily interpretable as the per capita birth rate when there is exactly 1 individual in the population, in other words, in the absence of competition. In reality, we never see birth rates this large. However you can estimate it by observing how birth rates vary with population size. It is also possible to estimate it from life history data if we know the maximum limits of birth rates under ideal conditions. We can work out what the population consequences of this simple assumption are by substituting (5.1) into (4.1) \\[\\begin{equation} N&#39;=\\left(\\frac{a}{N}-d\\right)N=\\frac{aN}{N}-dN=a-dN \\tag{5.2} \\end{equation}\\] and assuming that immigration and emigration are zero. Instead of a constant rate of growth, we have a rate of growth that decreases linearly with \\(N\\), the population size. That is, (5.2) is the equation of a line, with a y-intercept of a, and a slope of –d. When the population is small, the growth rate \\(N&#39;\\) is positive, so the population grows. Eventually the population gets large enough that the growth rate \\(N&#39;\\) becomes negative. Populations that large or larger will shrink because the population growth rate (\\(N&#39;\\)) is negative. The point that divides populations that grow from populations that shrink is the x-intercept of our linear equation. The x-intercept is the value of \\(N\\) that yields \\(N&#39;\\) = 0 when substituted into (5.2). We can find this value by setting (5.2) equal to zero, and solving for \\(N\\). \\[\\begin{equation*} \\begin{split} N&#39;=a-dN &amp; = 0 \\\\ dN &amp; = a \\\\ N^* &amp; =\\frac{a}{d} \\end{split} \\end{equation*}\\] In mathematical terms, this point is called an equilibrium point, because if the population ends up exactly at \\(N^*\\), it will remain there indefinitely. It will stay there in the absence of other forces (sound familiar?) because the rate of change in \\(N\\) is zero at that point. In ecological terms, this point is known as the carrying capacity, the size of the population that can be supported by the present environment. This is also known as the ecological carrying capacity, and is also often given the symbol \\(K\\). There’s another equilibrium point in this model. If a population reaches \\(N = 0\\), then the growth rate is 0 and the population will remain at that point indefinitely. However, it is not a stable equilibrium point, because any slight perturbation that makes \\(N &gt; 0\\) will lead to positive population growth. This is quite different from the equilibrium point at \\(N = K\\). There positive perturbations to \\(N\\) lead to negative growth rates (birth rate lower than death rate see Figure 5.2), while negative perturbations to \\(N\\) lead to positive growth rates (birth rates higher than death rates). These feedbacks will tend to keep the population in the vicinity of the carrying capacity. 5.1 The many flavors of carrying capacity The term carrying capacity gets used loosely in many population management contexts. In addition to the very specific sense of the population size at which the rate of change is zero, there is at least one other meaning that is important. The social carrying capacity is the maximum population size that is tolerable to human populations living in the same area. This term usually gets used in the context of over-abundant populations. A population is over-abundant when (at least some) humans feel there are too many of a species. This is an area that is fraught with potential for conflict. Inevitably, there will be groups of people that disagree on what the suitable population size is for a certain species. The most prominent example of this in the national news is the Northern Rocky Mountain population of grey wolves. Conservationists want more wolves, while ranchers and others that live in the vicinity want fewer – ideally no – wolves. The only place where wolves are likely to reach their ecological carrying capacity is in Yellowstone National Park. Outside the park, social pressure has lead to harvesting, which in turn will lead to lower population size. Figure 5.2: Per capita birth and death rates for a model with birth rates affected by scramble competition So what do the dynamics of this model look like? Whether we start above or below \\(N^*\\), the population decays (grows) towards the equilibrium point (Figure 5.3). The important assumption here is that the parameters \\(a\\) and \\(d\\) are constant over time. The population growth rate changes as population size changes, but the relationship between growth rate and population size is constant. We say this population is regulated, because it neither grows towards infinity nor decays towards zero without limits. Figure 5.3: Growth trajectories implied by (5.2) for \\(a=0.8\\) and \\(d=0.2\\). Early mathematical ecologists formulated models like (5.2), and referred to the relationship between population size and population growth rate as density dependence. As mathematicians, these early theoreticians were particularly fond of straight lines (the math is easier), and if both per capita death and birth rates are straight lines (Figure 5.4) then we obtain \\[\\begin{equation} N&#39;=rN\\left(1-\\frac{N}{K}\\right) \\tag{5.3} \\end{equation}\\] where \\(r\\) is the intrinsic rate of population growth and \\(K\\) is the ecological carrying capacity. This is commonly known as the logistic equation. Pierre-Francois Verhulst first published this model in 1838 after reading Thomas Malthus An Essay on the Principle of Population (which also inspired Charles Darwin). It was “rediscovered” by Raymond Pearl and Lowell Reed in 1920, which is why you’ll sometimes see this described as the Pearl-Verhulst equation. Alfred Lotka published it in 1925 and called it the Law of population growth. Figure 5.4: Per capita birth and death rates as functions of density as assumed by the logistic equation. In the graphical depiction of the logistic equation (Figure 5.4), the carrying capacity \\(K\\) is given by the intersection of the two lines. The intrinsic rate of growth, \\(r\\), is the difference between the birth rate and death rate when the population size is zero, or simply the difference between the y-intercepts of both lines. The logistic equation is usually shown as I’ve done here, with both death and birth rates changing with population size. However, that’s not really essential. As long as the two lines cross at some positive population size, you will see logistic dynamics. So the per capita birth rate can be independent of density (ie. a flat line) as long as the per capita death rate is increasing. In addition, although it is common to talk about resource competition as the source of density dependence, it is not the only possibility. Changes in death rates can also arise from changes in predator behavior. If a biotic or abiotic process affects birth or death rates differently at different population sizes, it can lead to density dependent population growth. The logistic equation (5.3) describes the rate of change in continuous time. This is mathematically convienent, but not necessarily a good approximation of population growth in temperate climates. Temperate climates, or seasonal climates, generally have very distinct periods when populations give birth, migrate, and die. In addition, it is easier to use these equations in discrete time (projecting one year from the previous year) in readily available software like R. Continuous time models require specialized software and/or great mathematical ability to obtain solutions. We need to move from (5.3) to something like \\[\\begin{equation*} N_{t+1} = N_t f(N_t) \\end{equation*}\\] where \\(f(N_t)\\) gives the net per capita rate of population growth. If \\(f(N_t) = \\lambda = e^r\\), then we have simple geometric population growth, which is the discrete time version of exponential growth. If \\(f(N_t)\\) is not constant, but depends on \\(N_t\\), as in logistic population growth, we have to do something more complex. One approximation that works quite well is to assume that the population grows exponentially at a constant rate for the next time step (I’ll use 1 year as the time step). At the end of the year, the growth rate is reset to a new value determined by the new population size. That is, assume that the growth rate changes between years, but not within a year. In that case \\[\\begin{equation} N_{t+1} = N_t f(N_t) = N_t e^{r\\left(1-\\frac{N_t}{K}\\right)} \\end{equation}\\] This is called the “Ricker equation” after the fisheries biologist that first worked it out. The dynamics of this equation for \\(r&lt;2\\) are very similar to the continuous time logistic equation. The population steadily approaches \\(K\\) from above (if \\(N_t &gt; K\\)) or below if (\\(N_t &lt; K\\)). However, the dynamics of the Ricker equation have some extra wrinkles. For example, if \\(r\\) is large enough it is possible for \\(N_{t+1} &gt; K\\) even though \\(N_t &lt; K\\). In other words, the population can overshoot the carrying capacity. This leads to a series of oscillations around \\(K\\), which gradually damp down in amplitude (Figure 5.5). When \\(r &gt;= 2\\) the dynamics of the Ricker equation become very bizarre. Wildlife populations will rarely have intrinsic growth rates that large, so we won’t worry about that now. From a practical perspective, the best property of the Ricker equation is that it will not predict future population sizes less than or equal to zero. Figure 5.5: Behavior of the discrete time Ricker model for two different values of \\(r\\). \\(K=1\\) and \\(N_0 = 0.1\\). 5.2 Density Dependent or Independent? The 20th century witnessed a vitriolic debate over the nature of population regulation. On the one hand were ecologists who thought that intraspecific competition led to changes in birth and death rates implied by the logistic equation. This was the “density dependent school,” epitomized by British ornithologist David Lack. Lack observed birds’ nests, and in particular the number of baby birds that successfully fledged. In the populations that he studied the number of fledgelings typically decreased with population size, and hence would lead to a regulated population as predicted by the logistic equation. On the other hand were ecologists who thought that the effects of the environment, including weather and food resources, were paramount. This “density independent school” argued that populations were affected more by abiotic factors, and that the effects of these abiotic factors on populations did not depend on the population size. Herbert Andrewartha and Charles Birch, two Australian scientists who worked on insects, were the chief architects of this view. They had followed the numbers of thrips and aphids on a rose bush outside their office building for many, many years, and observed that the population size was strongly influenced by the amount of rainfall, and not by the population size in previous weeks. (That bush was outside my office when I worked at the Waite campus of the University of Adelaide. It still has thrips and aphids. There should be a shrine there.) So who is right? To some extent, both camps are, but the density independent camp is more wrong. It is quite easy to show that a population that is subject to random shocks (ie. weather), but otherwise has a constant population growth rate \\(r\\), will eventually either decay to zero (if \\(r&lt;0\\)) or grow to infinity (if \\(r&gt;0\\)). In order for a population to persist at some finite size, the rate of population growth must eventually decrease with increasing population size. However, it does not have to exactly match the logistic equation. What confused many, many ecologists thoughout the 20th century was the derivation of the logistic function from the assumption that per capita birth and death rates were straight lines. This makes the math easy, but it isn’t necessary to have a regulated population. The functions \\(b(N)\\) and \\(d(N)\\) can, and do, have many different shapes (Figure 5.6). In particular, if there is a portion of the curve that is flat, then when a population is within that region birth rates remain constant with apparently no relationship to population size - density independence! However, if a population grows large enough, density dependence will, indeed must, appear. Figure 5.6: The mean number of independent young as a function of the number of breeding females in the Mandarte Island Song Sparrow population. In 1985 some females received supplementary feeding, which increased their reproductive output. The rate of change in reproductive output is lower, almost flat, when the number of females is less than 40. Adapted from Arcese and Smith (1988) So in what sense were Andrewartha and Birch right? What they did was point out that the environment matters. The parameters in the logistic equation (or any other population model), \\(r\\) and \\(K\\), are not constant with time. Rather, they can vary randomly from one time period to the next. In other words the environment is stochastic. 5.3 Stochasticity The word stochastic comes from Greek, where its original meaning was “to aim at a mark, guess” (OED online, Feb 17 2012). The modern meaning of a quantity “that follows some random probability distribution or pattern, so that its behavior may be analyzed statistically but not predicted precisely” did not come into use until the 20th century. In population dynamics the idea that the number of individuals in a population could be treated as a random variable in models can be traced to J.G. Skellam’s book chapter “The mathematical approach to population dynamics” in 1955 in The numbers of man and animals. Prior to this point stochastic variation in numbers was attributed to sampling error alone. The fundamental process that makes population dynamics stochastic is that births and deaths occur at random. Not every individual female in a population gives birth to the same number of individuals in a period of time. Whether or not an individual dies is fundamentally not predictable. Instead, each of these events occurs with some probability. We call this source of stochasticity in populations demographic stochasticity. Although physicists cannot predict the behavior of individual particles in a gas, the statistical properties of many, many particles in a container are quite predictable. For example, as the temperature of a gas in an enclosed container rises, so does the pressure (Ideal gas law). Similarly, ecologists cannot predict the fate of individuals but changes in the number of a whole group of individuals can be quite predictable. By analogy with the dynamics of particles, the larger the group, the more predictable the changes in group size. Demographic stochasticity has larger effects on smaller populations. The development of an exponentially growing population with demographic stochasticity follows the development in Nick Gotelli’s book “A Primer of Ecology”. We’re now thinking of births and deaths as discrete events that occur sequentially. So a population might experience a sequence like BBDBDBBBDDBD over some short period of time. Alternatively, it could be DBBDDDBDBBBB. The point is that we now cannot predict exactly how many births or deaths will occur over some span of time. If the per capita birth rate, \\(b\\), is constant over time then the probability of a birth across any span of time is \\(p(b)=\\frac{b}{b+d}\\), because we will either have a birth or a death. Similarly the probability of a death becomes \\(p(d)=\\frac{d}{b+d}\\). As a result, we can no longer talk about the population size at time \\(t\\), but only about the statistical properties of the distribution of population size, the mean and the variance. The mean is easy - it is just the same formula as for exponential growth. \\[\\begin{equation} \\bar{N_t}=N_0 e^{rt} \\tag{5.4} \\end{equation}\\] Calculating the variance depends on whether or not \\(b\\) and \\(d\\) are equal. Usually they will not be equal, and then the variance of \\(N_t\\) is \\[\\begin{equation} \\sigma_{N_t}^2=\\frac{N_0 \\left(b+d\\right) e^{rt}\\left(e^{rt}-1\\right)}{r} \\tag{5.5} \\end{equation}\\] There are several important things to notice about this equation. First, the variance increases with population size, as we’ve observed before (See box Taylor’s Law). This is different from sampling variation however – this is fundamental unpredictability about future population sizes. The variance also increases with the absolute magnitude of the per capita birth and death rates, because of the term \\(b+d\\). That is, a population with \\(b=0.1\\) and \\(d=0.05\\) will be less variable than a population with \\(b=1.1\\) and \\(d=1.05\\), even though both populations have a growth rate \\(r=0.05\\). The second population has much higher rates of turnover, and so a greater risk of a run of births or deaths causing the population to deviate from the average. Finally, the variance will increase with time, because of the terms containing \\(e^{rt}\\) (Figure 5.7). If the per capita birth and death rates are equal, \\(r = 0\\), and the variance is simply \\[\\begin{equation*} \\sigma_{N_t}^2=2 N_0 b t . \\end{equation*}\\] In the presence of stochasticity, it is quite possible for populations with positive population growth rates to remain flat or even decrease periodically. Figure 5.7: Exponential growth with demographic stochasticity. Thin lines are 20 replicate time series with \\(N_0 = 20\\), \\(b=0.1\\), and \\(d=0.05\\). The thick line is the expected population size, and the grey polygon shows the confidence limits on population size. Notice that trajectories can be above or below the average. The model in (5.4) assumes that the per capita birth and death rates remain constant across time. We know that is not true; some years are wet, some dry. Some years are hot, others cold. In some years predators are abundant, while in a couple of years they may be much less abundant. To account for this variability statistically, we can think of the per capita rates \\(b\\) and \\(d\\) as stochastic; that is, their values follow some probability distribution so that the statistical properties are known, even if the particular rates for a particular year are not predictable. Consider a population of northern Bobwhites (Colinus virginianus). We know that in years with particularly cold winters with late deep snow, Bobwhites have very low survival (Figure 5.8). These sorts of years do not occur often, but when they do they have a big effect on the dynamics of the population. We can have a death rate in good years, \\(d_{good} = 0.2\\), and a death rate in bad years \\(d_{bad} = 0.8\\). In order to predict the dynamics of this population on average, we also need to know the probability that a bad year occurs, let us call that \\(p_{bad} = 0.05\\), so that bad years happen, on average, once every 20 years. Figure 5.8: The mean number of whistles per stop over time for Northern Bobwhite (Colinus virginianus) in Nebraska. 1983 was a very late, hard winter with deep snow. Image courtesy of noflickster, creative commons attribution, noncommercial, sharealike. ## Warning in rbar * t: Recycling array of length 1 in array-vector arithmetic is deprecated. ## Use c() or as.vector() instead. ## Warning in (b - d) - rbar: Recycling array of length 1 in vector-array arithmetic is deprecated. ## Use c() or as.vector() instead. ## Warning in 2 * rbar * t: Recycling array of length 1 in array-vector arithmetic is deprecated. ## Use c() or as.vector() instead. ## Warning in rsigma2 * t: Recycling array of length 1 in array-vector arithmetic is deprecated. ## Use c() or as.vector() instead. Figure 5.9: Exponentially growing population with environmental stochasticity. The dashed lines are 20 replicate simulations with \\(b=0.3\\) and \\(d=0.2\\) in 19 out of 20 years, and \\(d=0.8\\) in 1 out of 20 years. The solid line is the expected population size for \\(\\bar{r} = 0.07\\), and the grey polygon is the confidence limits for the population given \\(\\sigma_{N_t}^2=0.017\\). \\(N_0 = 200\\) for all simulations. This type of variation in population growth rates generates some odd looking dynamics (Figure 5.9). The population grows exponentially, except for those years in which a catastrophe (large death rate) occurs. However, after the catastrophe the population resumes growing at the same rate. Population size at some point in the future depends on the number of catastrophes that have occurred, but not when they occur. If we calculate an average rate of growth \\(\\bar{r}\\), and a variance in the rate of growth, \\(\\sigma_r^2\\), we can approximate the future growth of the population with \\[\\begin{equation} \\begin{split} \\bar{N_t} &amp; =N_0 e^{\\bar{r}t} \\\\ \\sigma_{N_t}^2 &amp; =N_0^2 e^{\\bar{r}t}\\left(e^{\\sigma_r^2 t}-1\\right). \\end{split} \\tag{5.6} \\end{equation}\\] For the catastrophe model this is very approximate, because it assumes that growth rates follow a normal distribution, and thus the population can be larger than a population that never experiences a catastrophe. However, the idea that \\(r\\) could have a normal distribution is not unreasonable. It is just a different kind of environmental variation. As with demographic stochasticity, the key things to recognize are 1) the variation in population size increases with time, and 2) the mean growth trajectory is not likely to be followed by any single real population. This last point bears repeating. When we evaluate an ecological forecast by comparing a predicted population and confidence limit with a real population time series, we should not expect the real population to exactly follow the mean. We should be happy if the trajectory is mostly within the confidence bounds for the forecast. Although we’ve modelled environmental and demographic stochasticity separately above, in reality they both occur in all real populations. Individuals are discrete, and hence demographic stochasticity matters. Birth and death rates are not constant, and hence environmental stochasticity matters. So which approximation should we use? It depends largely on the size of the population. When a population is very small (say \\(N_0 &lt; 50\\)) the probability of reaching a small size, or even zero individuals, becomes larger and larger. In this circumstance it is very important to account for the discreteness of individuals, and include demographic stochasticity. As population size increases the probability of extinction due to chance alone becomes smaller. When populations are large, the effect of demographic variation in birth and death rates can be well approximated with a normal distribution. We can safely use the environmental stochasticity model to forecast the combined effects of demographic and environmental stochasticity. The variation that we see in a real time series includes sampling error as well as demographic and environmental variation. In an ideal situation we have some estimate of the sampling error from our abundance estimation, but we often do not. The error bars in Figure 5.8 come from averaging across different “routes” in Nebraska. The variance in counts from replicated samples includes the effects of both sampling error, demographic and environmental stochasticity, and we cannot seperate the three sources of stochasticity. Although I introduced the idea of stochasticity using the exponential model, it is equally possible to include stochasticity into a logistic model. The exponential model is convienent because there are nice, compact formulas for the mean and variance of population size. The same cannot be said for the logistic model. It is also a bit more challenging to think about how to include environmental variation – does it affect \\(r\\), \\(K\\) or both? Although you may see models that allow \\(r\\) or \\(K\\) to vary independently with time, the best answer is in fact both. Also the changes are correlated, not independent. To see why, consider Figure 5.10. Consider 2 types of years. In good years the per capita death rates are lower, while in bad years the per capita death rates are higher. In bad years, both \\(r\\) and \\(K\\) decrease relative to good years. That means that there is a positive correlation between \\(r\\) and \\(K\\) - small \\(r\\) means small \\(K\\) and vice versa. This correlation arises because \\(r\\) and \\(K\\) are derived parameters - they are the result of combining biological processes at the individual level into population scale parameters. Figure 5.10: Per capita birth and death rates as functions of density as assumed by the logistic equation. The death rate can be either good or bad, depending on the year. 5.4 Trends over time Stochastic variation in the vital rates of a population is one kind of change over time, but the sections above have assumed that the stochastic variation is stationary. A stationary distribution has statistical properties, ie. mean and variance, that are not changing with time. That is, even though the death rate in Figure 5.10 is different in each year, the mean death rate averaged across several years is not changing. What happens if the mean death rate does change with time? Figure 5.11 shows some of the possibilities. The most important feature is that the population size lags behind the changes in \\(K\\). This is easiest to see in the deterministic population model. \\(K\\) is decreasing linearly with time, but the population size does not decrease at the same rate initially. As the discrepancy between the population size and \\(K\\) gets larger, the negative pressure on population size increases until the point at which the population is decreasing just as fast as \\(K\\). After that point in time, both \\(K\\) and the deterministic population size decrease at the same rate; the lines on the graph are parallel. It turns out that the lag time between the start of the change in \\(K\\) and the point at which population size is decreasing equally quickly depends on the magnitude of the intrinsic growth rate \\(r\\). A population with large \\(r\\) will be able to track changes in \\(K\\) more closely, with a very short lag time. A population with small \\(r\\) will respond more slowly to changes in \\(K\\). The other feature is that even small amounts of environmental stochasticity can create misleading trends over short time periods. For example, despite a decrease in carrying capacity, there are several periods where the population size subject to environmental stochasticity is relatively constant, or even increasing for a time. The trend in carrying capacity is influencing the average population size, but any particular time series will not necessarily track that average perfectly. The moral of the story is to be very careful about drawing grand conclusions from short time series. Up until very recently it was common for earth scientists and ecologists to talk in terms of stationary systems – climate systems, ecosystems, and the like. However, given increasing awareness of large scale climate change, we should expect that systems are not stationary, and that trends in underlying vital rates should be the norm, rather than the exception. In addition, here in the Great Plains the landscape itself has not been stationary. Land use affects wildlife populations, and changing agricultural economics has led to large shifts in available habitats across the last half century or so. We have little information of sufficient extent in space and time to really understand what is going on. Figure 5.11: Ricker dynamics with and without a trend in carrying capacity. \\(N_0 = K_0 = 1000, r=0.2\\), and the rate of change in \\(K\\), \\(k = - 10 / year\\). The variance in population growth rates is \\(\\sigma^2 = 0.0025\\). The left hand panel shows both deterministic and stochastic population growth. 5.5 Glossary Scramble competition all individuals in the population have equal access to resources. Regulated population a population that neither grows nor decays without limits. "],
["chap-harvesting.html", "Chapter 6 Harvesting and Control 6.1 Compensatory and Additive Mortality 6.2 A Better Model For Harvesting Wildlife 6.3 What about control? 6.4 A “rule of thumb” for harvest and control 6.5 Exercises", " Chapter 6 Harvesting and Control Up to this point, I was content to let wildlife populations be. Now I want to mess around with them. In a closed population there are two options: alter per capita birth rates or per capita death rates. Both types of manipulation will result in changes to population size, which is often the objective of management. But some manipulations of death rates have a side effect of producing a flow of dead animals, ie. a harvest. This flow of dead animals can also be an objective of management if the animals have value after death. Value can arise from many things, including furs, meat, and trophies. Managers have to make decisions about whether, and how, to regulate harvest of wildlife populations. The same models of populations that help with harvest regulation can be used to make decisions about control of nuisance populations. Throughout this chapter I will talk about harvest regulation, but control of wildlife populations is just harvest with a different objective. Eurasian Lynx in Norway Everything is better with an example, and there is a great deal of information on the management of Eurasian Lynx (Lynx lynx) in Norway (Linnell et al. 2010). Lynx were widespread throughout Norway at the beginning of the 19th century. As human populations grew, so did negative interactions with lynx. In 1846 the Norwegian government issued a bounty on lynx – essentially paying for dead lynx – which remained in place until 1980. Throughout that period the allowed methods for taking lynx were reduced, and populations recovered somewhat from their lowest levels in the 1940’s. In 1976 lynx were restricted to two small remnant populations, and were listed on the CITES Appendix II. After the bounty was removed, hunting was still allowed. The season was restricted starting in 1982, but there were no restrictions on the number of lynx taken. Lynx populations began to recover according to records of the number of lynx harvested. Managers regarded these records as relatively accurate because there was little incentive to poach, given the lack of restriction on harvest. Throughout the 1980s and early 1990’s the number of lynx taken annually was less than 50. In 1992 public outcry over the harvest of a mother and both of her kittens by a hunter led to a temporary cessation of all hunting. Finally, in 1994 a regulated harvest with a restricted season and a nationwide quota of 47 animals was allowed. In addition, a program that compensated livestock owners for losses to lynx and other large carnivores was put in place. Population size of lynx has been estimated using consistent methods since 1996 (Figure 6.1). Quotas were greater than 25 to 30% of the estimated population size, and populations declined steadily until 2004. Since 2004 quotas have been kept less than 20% of the estimated population size. Annual variation in the number compensation payouts have mirrored variation in estimated population size. Figure 6.1: Estimated number of lynx in Norway, the annual hunting quota, and the # harvested since 1994. 6.1 Compensatory and Additive Mortality A common paradigm in wildlife science is the distinction between compensatory and additive mortality. This distinction is usually made when referring to anthropogenic, or human-caused, mortality such as harvest. If anthropogenic mortality is additive, it means that the total death rate is the sum of anthropogenic mortality and natural mortality. If anthropogenic mortality is compensatory, it means that natural mortality is reduced such that the total death rate is unchanged. Clearly compensatory mortality is very useful, even essential, if the objective is to sustainably harvest a population. A typical depiction of the distinction is shown in Figure 6.2. As anthropogenic mortality increases the probability of survival decreases in the additive case. Any degree of extra mortality reduces survival probability. In contrast, as anthropogenic mortality is increased in the compensatory case, the probability of survival does not decrease, at least initially. Eventually the extra mortality becomes too great, and the survival probability begins to decrease. The rate of decrease is not specified, but it will not lead to survival probabilities that are less than those under the additive case. I call this the “standard model” of wildlife harvest. Typically the assertion is that harvest rates up to the point of compensation are sustainable. The distinction between compensatory and additive mortality was a central part of debates over waterfowl harvest in North America throughout the 1970’s, 1980’s and beyond. Figure 6.2: Hypothetical survival probabilities for individuals under additive and compensatory mortality. The mechanism that creates compensatory mortality is a density dependent change in natural mortality rates, like that assumed under the logistic population model. Anthropogenic mortality reduces the population size, and the remaining individuals experience lower natural mortality. However, the standard model is misleading for at least two reasons. First, as shown in the previous chapter, populations that are regulated by density dependence may not have changes in mortality. Increasing density could reduce birth rates instead of increasing death rates. Or both birth rates and death rates could change. Second, population size must decrease in order for compensatory decreases in death rates to occur. The standard model does not show this effect because it plots survival against harvest rate, not population size (Figure 6.2). 6.2 A Better Model For Harvesting Wildlife We can obtain a much richer understanding of what is going on with wildlife populations under harvest, or indeed, under any kind of increased anthropogenic mortality by using the graphical techniques from Chapter 5. I will start with a plot of the population growth rate versus the population size (Figure 6.3). Ignoring environmental and demographic stochasticity, you know that the population will reach a stable equilibrium at \\(K\\). At that point, the population growth rate is zero. Now imagine that you harvest a fixed proportion of the population, \\(h\\). This is a straight line on our graph with a slope of \\(h\\), because the amount harvested (and hence the change in \\(N\\)) increases with population size. This creates a new equilibrium point where the population growth rate is equal to the harvest rate, and the total rate of change is again equal to zero. The most important point to recognize is that this new equilibrium point is \\(&lt;K\\). Harvesting reduces population size. Always. A decrease in population size under harvest does not necessarily mean the harvest is unsustainable. For a logistic population, a fixed proportional harvest always has a positive equilibrium population unless \\(h &gt; r\\). A harvest that large has no positive intersection with the population growth rate curve, and thus the only equilibrium point is \\(N=0\\). Figure 6.3: Population growth as a function of population size. The thin lines show the effects of a fixed proportion of harvest. As the per capita harvest rate h increases, the population decreases. What happens to the number of animals actually harvested? A little bit of algebra will get us the answer. We start with the discrete Logistic model with an additional term for the harvest rate, and then do some algebra to figure out how many animals are harvested at the level \\(h\\). The math is identical for a continuous time model, but you can use this equation in R. This equilibrium harvest is \\(H^*\\). \\[\\begin{equation} \\begin{split} N_{t+1} &amp; = N_t+N_t r\\left(1-\\frac{N_t}{K}\\right)-hN_t \\\\ N^* &amp; = K^*\\left(1-\\frac{h}{r}\\right) \\\\ H^* &amp; = hN^* = hK\\left(1-\\frac{h}{r}\\right) \\end{split} \\end{equation}\\] These equations are shown in Figure 6.4. There are two important features to note. First, as harvest rate increases, the heavy dashed line rotates up and to the left, and the equilibrium population size decreases. Second, as harvest rates increase, the total harvest first increases, reaches a maximum value at \\(r/2\\), and then decreases. This means that for any particular total harvest \\(H\\), there are two different harvest rates that will achieve it. The higher harvest rate corresponds to an equilibrium population size that is less than half the carrying capacity. Although this equilibrium is stable, populations in this state are described as “overexploited”, because there is a lower harvest rate that will produce the same total yield at a higher equilibrium population size. In the overexploited state, increased harvest leads to decreased yield. The harvest rate that generates the maximum yield, \\(r/2\\) is also called the Maximum Sustainable Yield, or MSY. The total amount harvested at that point is \\(rK/4\\). Figure 6.4: In the left panel the population rate of change (solid line) for the discrete Logistic model with a fixed proportional harvest (dashed heavy line) is plotted against population size. The parameters are \\(r=1\\), \\(h=0.3\\) and \\(K=1000\\). The vertical dashed line is the population size under harvesting pressure. In the right panel the total harvest \\(H^*\\) is plotted against the per capita harvest rate. The vertical line shows the maximum harvest occurs at \\(r/2\\). Epitaph for MSY Peter Larkin wrote the following poem in his 1977 paper outlining why aiming for MSY was a bad way to manage a commercial fishery. Here lies the concept, MSY. It advocated yields too high, And didn’t spell out how to slice the pie. We bury it with the best of wishes, Especially on behalf of fishes. We don’t know yet what will take it’s place, but we hope it’s as good for the human race. 6.2.1 Fixed quota harvesting What are some other ways we could harvest a wildlife population? It is quite typical for recreational harvests to be regulated by controlling the total number of animals harvested. I call this a fixed quota harvest strategy. If the quota is always filled (a big assumption), then the per capita harvest rate is \\(\\frac{Q}{N}\\), and the population harvest rate is a flat line (Figure 6.5). As before, there is an equilibrium point \\(K&#39;\\) with a positive amount of harvest that is just a bit less than \\(K\\). This equilibrium point is stable. When the population size is greater than \\(K&#39;\\) harvest rate exceeds the population growth rate, and the population will decrease back toward \\(K&#39;\\). If the population size is lower than \\(K&#39;\\), then the population growth rate is greater than the harvest rate, and the population will grow. Figure 6.5: Population growth as a function of population size. The thin line shows the effects of a fixed quota \\(Q=0.2K\\). The thin dashed line is a fixed quota of \\(Q=0.9K\\). The parameters of this curve are \\(r=3\\) and \\(K=1\\). What about the point \\(K&#39;&#39;\\)? It is an equilibrium point because the harvest rate and population growth rates are equal. However, \\(K&#39;&#39;\\) is not stable. If the population is reduced below \\(K&#39;&#39;\\), the harvest rate exceeds the population growth rate, and the population will decrease. The next equilibrium point is \\(N=0\\), which in this situation is a stable point. If the population size falls below \\(K&#39;&#39;\\), and harvest is maintained at \\(Q\\), harvesting will drive the population extinct. The other way to drive a population extinct with fixed quota harvesting is to set the quota \\(Q &gt; rK/4\\). Recall that the peak of the population growth rate curve in Figure 6.5 is \\(rK/4\\). This quota corresponds to a flat line that is higher than the parabola representing the population growth rate. In other words the harvest is greater than the population growth rate for all population sizes. Harvesting a fixed proportion is considered safer than a fixed quota, because a fixed proportion doesn’t lead to the formation of an unstable limit, unless the harvest rate is very high (\\(h &gt; r\\)). As the population size decreases, so does the harvest. However, the difficulty with a proportional harvest is that it requires an accurate estimate of population size. If your estimate is high, \\(\\hat{N}&gt;N\\), then actual proportional harvest is higher than assumed. That means that the equilibrium \\(K&#39;\\) is closer to the peak of the population growth rate curve, and \\(h\\) is closer to the limit \\(r\\) that will cause the population to go extinct. 6.2.2 Fixed Effort harvest strategy It is worth asking if there is a way to get a “safe” harvest without knowing the population size exactly. Instead of regulating the harvest directly, it is also possible to regulate the effort that harvesters put in. The assumption is that harvest rate increases with the amount of effort. This is often done by limiting the duration of the period harvesters are allowed to operate, or by limiting the gear that they can use. By restricting effort, the harvest rate adjusts naturally to the abundance of the population without knowing the population size. The simplest model of effort is \\(h = qE\\), where \\(E\\) is measured in units of effort (days, fishing lines, etc.) and \\(q\\) is a “catchability coefficient” which describes how harvest rate changes with effort. If effort is fixed over time, and \\(q\\) remains constant this model will have the same dynamics as a fixed proportional harvest, but does not rely on an estimate of population size. Effort can change in different ways. Many commercial fisheries have collapsed in part because of failures to account for “effort creep”. For example, in the 1950’s a prawn fishery developed in the Gulf of Carpenteria on Australia’s northern coast. A typical boat was less than 50’ long, and could tow a trawl net that covered 30-40’ of bottom. The fishery was regulated by the number of days and the number of licenses (units of boat-days). By the time I worked on the Northern Prawn Fishery in 2002, a typical prawn trawler was &gt; 200’ long and capable of towing 4 trawls simultaneously! Add to that GPS locator capability, sonar depth sounders, and you have a machine vastly more efficient at harvesting than the boats of the mid twentieth century. A boat-day in 2001 was not the same as a boat-day in 1955. Effort creep leads to an increase in the catchability coefficient \\(q\\). Another example of the effect of controlling effort comes from Oregon’s sport harvest of cougars (Puma concolor). The history of Oregon’s cougars is similar to Norway’s Eurasion lynx; near eradication followed by a steady recovery. A regulated sport harvest has been in place since 1970 (Figure 6.6). For the most part, the harvest has been managed as a fixed quota which has rarely been filled. The biggest change in effort occurred in 1994 when the use of hounds to track and tree cougars was outlawed by a Ballot proposition (Period D in Figure 6.6). The rate of successful harvests plummeted. In response, the Fish and Game commission added mountain lions to a multi-species license package, which meant that many more hunters could opportunistically take lions even if they were not searching for them specifically. Eliminating hounds reduced the effectiveness of hunters (reduced \\(q\\)), so to maintain harvest rates the number of hunters was increased (increase \\(E\\)). Figure 6.6: Total statewide cougar harvest by hunters from 1970 to 2012, Oregon, USA. Grey rectangles indicate periods with major regulatory changes related to distinct harvest regulations. Points shaded grey are partial counts, as no records were kept for portions of the state. A: unknown regulations. B: increased quotas. C: All harvested animals submitted for aging, hounds allowed, species specific license. D: Hounds outlawed by ballot proposition. E: Addition of mountain lions to the sport-pac multispecies license. Adapted from Tyre et al. unpublished ms. 6.2.3 Revisiting Eurasian Lynx So how is the Eurasian lynx harvest managed? It seems to be a bit of a hybrid. There is a quota, but it is adjusted continuously as the harvest size changes as if attempting to maintain a fixed proportion. It is not clear what the target proportion was, because there is no fixed proportion. There is an additional wrinkle too — the quota is not filled each year. That is, there are some hunting opportunities that are not taken. This could arise either because of a lack of hunters, a lack of animals, or both. Variation in the degree to which harvest goals are achieved could be treated as another source of stochasticity in the population dynamics. One term used for this is implementation uncertainty (Bischof et al. 2012). This sort of variability is quite common in recreational harvest systems, and makes understanding them much more difficult than commercial harvest systems. In a commercial harvest you can rely on harvesters to aggressively maximize their economic benefit. The benefit to a recreational harvester for hunting is more complex. In addition to the obvious food, fur, or trophy, recreational harvesters just like to be outside. As a result, simple things like a particularly cold day or the scheduling of a popular football game can have an effect on the level of harvest. You might think the football game is a far-fetched example, but the effect has been noticed by Nebraska Game and Parks staff at deer check stations! Prior to 1994 the lynx harvest was averaging less than 50 individuals per year, even though it was unregulated. In 1996 and 1997 the quota was raised to more than double this level, and harvests also increased substantially. The population began to decline, but if our model is correct, this would be expected. Even if the quota had not been reduced in response to the decline, I would have expected the population size to level out at a new equilibrium. However, the quota was reduced steadily, until 2004 when it was reduced by a large amount. Subsequently to that the population began to increase again! I can only imagine the degree of angst and political wrangling that goes into these decisions. 6.2.4 Revisiting Additivity It is important to recast the additive mortality paradigm into our improved model. The charge of additive mortality is frequently used by political opponents of hunting to argue for reduced or eliminated harvest. The only kind of population that will not experience a compensatory increase in population growth rate is a population that is growing exponentially. In an exponentially growing population both the birth rates and the death rates are independent of population size (Figure 6.7). If the harvest rate is less than \\(r\\), then the population continues to grow exponentially, albeit at a slower rate. If the harvest rate is greater than \\(r\\), the population will decline exponentially. The only harvest rate that will keep the population from either increasing or decreasing is exactly \\(r\\). In the face of environmental stochasticity, estimation error and implementation uncertainty, this is impossible to achieve. That does not mean that harvest mortality is never additive. The additional wrinkle to keep in mind is that it is possible for the relationship between vital rates and population size to change over time. In particular, animals in strongly seasonal evnironments or that migrate long distances may experience very different environments at different stages of the year. Mule deer (Odoceilus hemionus) in Colorado show density dependent decreases in survival during severe winters, but not during mild winters (Bartmann, White, and Carpenter 1992, Unsworth et al. (1999)). Thus, the demographic response to a fall harvest will be different in years with mild vs. severe winters. In a year with a mild winter the fall harvest mortality will be more additive, with no compensatory decrease in mortality of individuals that survive harvest. Figure 6.7: On the left are per capita birth (b=0.6), death (d=0.1) and harvest rates (h=0.2). On the right is population growth as a function of population size. The thin lines show a proportional harvest at h = 0.2 or h=0.6. 6.3 What about control? Control is usually suggested when stakeholders are experiencing negative impacts from the species. Biologically, there is no difference between harvesting a population and culling a population to reduce damage. The only difference is the objective. Typically the objective of harvesting is to allow the total harvest to be as large as possible without driving the population extinct. In contrast, the objective in control situations is usually to reduce the population size as efficiently as possible. If the relationship between population size and impact is linear, then it is sufficient to express the objective of control in terms of population size. Although negative impacts usually decrease utility as population size increases, this decrease may not be linear. In that case it is important to express the objective directly in terms of the reduction in damage. This will result in control programs that are more effective (in the sense of reaching acceptable damage levels), and more efficient (in the sense of using the smallest amount of effort needed). Some of these possibilities are shown in Figure 6.8. Assuming that the relationship is proportional might lead to the population target shown by the dot. If the actual relationship is given by curve A, then this population level target will not reduce the damage to socially acceptable levels. In contrast, if the relationship between population size and damage is given by curve B, then the population objective will be inefficient, reducing the population much more than is needed to achieve a level of social acceptance. Figure 6.8: Some possible non-linear relationships between utility and population size. The dot labelled K is the current population size and level of damage. The thin solid line represents the assumption that damage is proportional to population size. The thin dashed line represents the socially acceptable level of damage. The dot represents a population level objective assuming that damage is proportional to population size, and the socially acceptable amount of damage is 30% of current levels. The other advantage of expressing control objectives directly in terms of levels of impact is that it helps expand the range of possible alternatives. If the conversation focuses exclusively on population levels, culling tends to be the first and often only option considered. Thinking in terms of population dynamics broadly opens up additional options. For example, rather than increase direct mortality rates it is also possible to reduce birth rates by various means. That changes the shape of the population growth rate curve, and can reduce equilibrium abundance. Thinking directly about damage levels opens up even more opportunities. For example, sometimes it is possible to move a population around a landscape, so that they spend more time in areas where they cause less damage. This can work by a combination of attractants (e.g. feeding stations) and/or dispersants (e.g. bangers). A particularly interesting strategy for keeping birds off of runways is to use trained falcons to increase the perceived risk of foraging on the runway. Joel Berger put forward the idea of a “landscape of fear” in his 2009 book “The better to eat you with: fear in the animal world” (Berger 2009). If animals are afraid of predators, they alter their behavior to reduce that fear. In Yellowstone National Park the reintroduction of wolves resulted in a change in behavior by elk herds to shift their foraging away from riparian areas favored by wolves. This had a beneficial side effect of reducing elk damage on fragile riparian vegetation. At the present time Rocky Mountain National Park is also experiencing high levels of vegetation damage by large populations of Elk. Rather than reducing elk populations by allowing hunting in the national park, perhaps another wolf introduction is warranted? 6.3.1 Predicting the response to management Regardless of how the manipulation is achieved, it is necessary to predict how much effort is required to achieve the objective. The improved model of wildlife harvest can be used for predicting the effects of control efforts that increase death rates. If the goal is to eliminate the population, and a fixed quota \\(Q\\) can be taken, then we need to set the effort at a high enough level to exceed the Maximum Sustainable Yield (Figure 6.5) which is \\(rK/4\\). Levels of effort that lead to per capita harvest rates less than \\(r/2\\) will reduce the population, but not drive it extinct. Specifying a particular quota in a control situation is usually unrealistic, because the effort required to reach the quota increases as population size decreases. In a commercial harvest situation effort will increase as population size decreases because that is how harvesters maximize profit. In a control situation someone or some agency must expend money to increase effort, and in practice that means there is an upper limit to effort. That in turn means that the “proportional harvest” model is more appropriate. In that case, driving a population extinct requires a per capita harvest rate \\(h&gt;r\\), or double the fixed quota harvest rate! Predicting the effect of a change in birth rates needs the underlying model of population regulation to be specified (see Chapter 5). If we do something that affects birth rates equally at all population sizes, then the per capita birth rate curve shifts downwards (Figure 6.9; green line). This reduces both \\(r\\) and \\(K\\) in the logistic equation, and leads to a smaller population size. It is also possible that some management actions affect individuals more in larger populations. For example, reducing the total amount of habitat will have this effect, leading to a steeper decline in the per capita birth rate with increasing population size (Figure 6.9; blue line). This reduces \\(K\\) but not \\(r\\). Driving a population extinct by reducing birth rates requires enough effort that the entire per capita birth rate curve drops below the per capita death rate curve. Thus it is not necessary to reduce birth rates to zero, but rather get them lower than per capita death rates at all population sizes. Figure 6.9: The effect of reducing per capita birth rates when a population grows according to the logistic equation. Blue: increasing effect of competition (e.g. reducing habitat, removing food). Green: decreasing reproduction at all densities (e.g. sterilizing individuals) Imagine you have a population of Koala (Phascolarctos cinereus) growing according to the logistic model, and currently at the maximum carrying capacity \\(K\\) (Figure 6.10). This population is disease free, and unfortunately the Eucaplytus population that provides all their food cannot sustain this population level for very long. Eventually the koalas completely defoliate and kill their food source unless their populations are maintained below a critical threshold (vertical red line in Figure 6.9). We have to choose between 3 different levels of culling (fixed annual quotas), and a surgical sterilization program that reduces birth rates at all densities, and thus reduces both \\(r\\) and \\(K\\). First, note that the only strategy that can reduce the Koala population below the critical threshold on its own is culling at level 3. Culling at level 1 or 2 reduces the population size somewhat, but these lead to stable equilibria above the critical threshold. The sterilization campaign reduces the population more, but still ends up at a stable population size above the critical threshold. The only stable point below the critical threshold is \\(K&#39;&#39;&#39;\\), which involves a combination of sterilization and culling at level 1. Sterilization changes the population growth curve, but does not immediately reduce population size. It is also very expensive. It may be cheaper and quicker (safer for the trees!) to cull at level 3 until the population is below \\(K&#39;&#39;&#39;\\), then reduce culling and start sterilization. If we could accurately count koalas, then a proportional harvest rate strategy that intersects the population growth curve to the left of the critical threshold would also yield a stable equilibrum. This point puts the population in the “overexploited” state. The risk is that having the harvest rate too high might put the population at risk of extinction. This risk is also present for the culling/sterilization strategy. Which strategy is “riskier” depends alot on the shape of those growth rate curves, how accurate the population estimates are, and how much environmental and demographic stochasticity is present in the population. Figure 6.10: Population growth as a function of population size. Strategies 1, 2, and 3 represent different fixed quota culls. Strategy 4 represents surgical sterilization of half the population. The parameters of the solid curve are \\(r=3\\) and \\(K=1\\); the dashed curve (sterilization) is \\(r=1.5\\) and \\(K = 0.5\\). The dots mark unstable equilibria; stable equilibrium points are labelled \\(K\\). Mid continental light geese The mid-continental populations of light geese (Lesser Snow Goose and Ross’ Goose) cause extensive damage to salt marsh habitats in their sub-arctic and arctic breeding grounds. In 1997 the Arctic Goose Joint Venture (AGJV) issued a report detailing the extent of the damage along the south coast of Hudson’s Bay, and recommended efforts to dramatically increase harvest by recreational hunters to reduce the population and protect these habitats. Populations of light geese began to increase after food availability in their wintering habitats increased with changes in agricultural practices, improving overwintering survival. In addition, numbers of hunters declined steadily throughout the 2nd half of the 20th century, reducing harvest mortality. Increased survival leads to an increase in population size even if the population is regulated by density dependence in birth rates (see chapter 5). The effect of increased goose foraging on fragile salt marsh habitats appeared to have a threshold response. Marsh vegetation could sustain low levels of foraging, but as foraging increased it reaches a point at which above-ground vegetation is completely removed. At that point, the feedback processes that keep soil salinity in check are removed, and soil salinity rises (Jefferies et al 2006). Marsh plants find it difficult or impossible to reestablish in high salinity soils, and the marsh remains as a mudflat even after goose foraging decreases. This is an interesting example because our utility, measured by the persistence of salt marsh vegetation, is high and relatively independent of goose populations until they reach high levels (Type B curve, Figure 6.8). However, once the threshold is passed, the utility curve will flip to a Type A curve, where utility is low until the population size is dramatically decreased! So what happened with the light goose population? The primary recommendation from the AGJV report was to increase mortality on adult geese during the spring when it was thought that mortality would be additive in effect (see the standard model of wildlife harvest). The degree of political support garnered from both American and Canadian governments was tremendous, and resulted in changes to the Migratory Bird Treaty Act to allow extended hunting seasons. However, despite longer hunting seasons, no bag limits and dramatically reduced regulations on how hunters could approach light geese by states and provinces, populations have continued to increase. Although the changes in regulations increased harvest rates for some breeding colonies, the increase was only temporary. This suggests that harvest is limited by effort, primarily the number of hunters rather than time or their individual effectiveness. As a result, increasing populations lead to decreases in the per capita harvest rate of geese. In the most recent assessment (AGJV 2012) the growth rate of light goose populations appears to be decelerating towards an equilibrium as we would predict from the improved model. The real question to be answered now is whether we humans are satisfied with the amount and distribution of salt marsh habitats that will be present at this new equilibrium. Living with large predators In both Norway and Oregon there has been an increase in the number of lynx and cougars. Elsewhere in North America cougar and wolf populations have also increased. These large predators readily attack and eat livestock, and as populations increase, the number of complaints from livestock producers increases too. In Oregon, a complaint from a livestock producer usually results in the death of the cougar on a depredation permit. At low populations cougars are restricted to areas of their preferred habitat in Oregon - undisturbed forested mountains. There are relatively few livestock in these areas. However as populations increase, young animals are increasingly forced out of preferred habitats into areas preferred for livestock production. This leads to an increase in complaints by producers. The utility curve from a producers perspective is probably sigmoid. At low population size producer utility is high, and it remains high until cougars are pushed out of wilderness areas. I suspect that once livestock predation begins to occur that a producer’s utility decreases very rapidly. Thus producers may want zero lions in their immediate vicinity, but be more tolerant of lions elsewhere. It is instructive to think about the utility curves of other groups of stakeholders. Groups like the Mountain Lion Foundation heavily criticize sport harvest of cougars in North America and wish to see larger populations of cougars. The MLF utility curve is low or even zero at low populations of cougars, and presumably increases with increasing population size. There does not appear to be a population size for cougars above which the utility for groups like the MLF levels off, or if there is, it is well above the present abundance and distribution of mountain lions. In recent decades a new group of stakeholders has emerged, urban dwellers whose homes begin to intersect expanding populations of cougars. David Baron documented this process along Colorado’s front range in his book “The beast in the garden” (Baron 2010). For people living in these areas it seems reasonable to think that their utility also increases with increasing populations of lions as for the MLF. However, it also seems likely that as populations get large enough that people feel pets, children, and themselves are at risk, that their utility begins to decrease. The fact that different subsets of society have different utility functions for the population size of cougars in a region is what makes large predator management controversial. Regardless of what is done, there is someone whose utility would be improved by a different choice. If that someone has sufficient political clout with their state or federal representative they will be better able to improve the outcome for themselves. Rather than bemoan the fact that large predator management is political, wildlife biologists need to embrace that fact, and accept that what we personally want may not be what happens. 6.4 A “rule of thumb” for harvest and control Managing wildlife populations is difficult even in the best of cases where managers devote substantial effort to data collection and analysis. Michael Runge and colleagues from the USGS developed a formula for determining “prescribed take level” for a species (Runge et al. 2009). The formula is derived from the logistic model of population growth, and is an extension of “Potential Biological Removal” formula written into the Marine Mammal Protection Act. Recall that the maximum sustainable yield for a population growing according to the logistic equation is \\[\\begin{equation} MSY = \\frac{r_{max}}{2}N^*. \\end{equation}\\] Harvest, or take, less than MSY will be sustainable in the sense of having a positive equilibrium population size \\(N^*\\). Runge et al. introduced a new “management objective” variable \\(F_0\\) which ranges between 0 and 2. The prescribed take level in year \\(t\\) is then \\[\\begin{equation} PTL_t = F_0 \\frac{\\tilde{r}_{max}}{2} \\tilde{N}_t. \\end{equation}\\] The \\(\\tilde{}\\) over \\(r\\) and \\(N\\) are there to remind you that these quantities are not known exactly. A management objective \\(F_0 = 0\\) corresponds to no allowed take, and the population will eventually stabilize at \\(K\\). A value of \\(F_0\\) greater than 1 corresponds to an objective of driving a population close to zero. The advantage of specifying take in this manner is that we do not have to estimate \\(K\\). All we need to know is the current population size, and the species potential for population growth at low population sizes. It is instructive to work backwards from harvest quotas set by management agencies (ie. PTL) using agency estimates of \\(N_t\\) and \\(r_{max}\\) to infer what sort of management objective is in place. Nebraska recently opened a mountain lion season for the Pine Ridge area in 2014. There were two short seasons, and no more than 2 males could be taken in each season. If a female was taken the entire hunting season would close. As it turned out, 3 lions (two males, one female) were harvested in total, out of an estimated population of 22. Ignoring uncertainty, and using \\(r_{max} = 0.28\\) from the literature, \\(F_0 = 0.97\\). This is approximately an MSY harvest, and the expectation is that the population will stabilize around half of the carrying capacity. There are a few reasons to think this might be an optimistic view of the allowed harvest. First, this simple model ignores sex structure of the population. A maximum of 4 males out of the male portion of the population alone is a harvest that will depress male population size much more than MSY levels. Second, the estimate of \\(r_{max}\\) comes from a remote area of Arizona. Nebraska lions experience other sources of anthropogenic mortality including roadkill and responses to human safety and cattle depredation. These reduce annual survival rates compared to more pristine wilderness areas, and would lead to lower values of \\(r_{max}\\). A third reason to regard these calculations with scepticism comes from the small number of animals involved. As we saw in Chapter 5, small populations experience stochastic fluctuations simply due to demographic stochasticity. 6.5 Exercises Assuming a harvested population is growing logistically, and the harvest is forbidden below K/2, and proportional to population size above K/2, draw a graph of the population growth and harvest rates. Be sure to label all the relevant points and unknown variables. If the per capita harvest rate in question 1 is increased to \\(h=r\\), does the population reach a stable equilibrium that is positive, and why? For the same population in question 1, what does the harvest curve look like for a fixed quota \\(Q\\) implemented as one licence per hunter. The probability that a hunter is successful increases linearly up to 1 at K/2. Draw a second harvest curve to show the effect of reducing the available time to hunt by 1/2. References "],
["chap-structured.html", "Chapter 7 Structured Populations 7.1 Age structure 7.2 Population Projection Matrices 7.3 A more complex example 7.4 Sensitivity and elasticity 7.5 Stage or size structured populations 7.6 Exercises", " Chapter 7 Structured Populations In previous chapters we’ve treated all individuals in a population as if they are identical and equally able to interact. However, this is manifestly not true. Individuals vary by age, size, gender, genotype and location, at a minimum. This heterogeneity affects everything in the fundamental equation. Individuals of different ages have different survival probabilities. Individuals of different genders have different probabilities of dispersing. Sometimes management actions affect old individuals, sometimes young ones, and sometimes only a part of the area occupied by a population. How can we incorporate this heterogeneity into our population models? 7.1 Age structure For long lived vertebrates, age is often the most obvious distinguishing feature. Older animals are often larger, have higher and more constant survival rates (Gaillard et al 2008), and produce offspring more successfully than younger animals. Age has obvious advantages as a means of distinguishing between animals, as an animal that survives from one year to the next is one year older. Nonetheless it is not trivial to estimate the age of free living vertebrates. Age structured population models typically project the population from one time step to the next. This is a discrete time approximation to the population dynamics. The time step can be anything, from a day (e.g. Tenhumberg et al 2011) to a year (e.g. Post van der Burg et al 2009). For long lived vertebrates in the temperate zone a year is a good place to start. The basic state variable for an age structured population model is a vector describing the number of individuals in each age class in a particular year \\(t\\) \\[\\begin{equation*} N_t=\\begin{bmatrix} n_{1,t} \\\\ n_{2,t} \\\\ \\vdots \\\\ n_{m,t} \\end{bmatrix}\\,. \\end{equation*}\\] The number of age classes is \\(m\\). For the moment ignore differences between genders. What does the number \\(n_{1,t}\\) represent? To answer this question, we have to specify a few more assumptions about the model. In particular, we have to describe the relationship between the census (the point in time where the individuals are counted), and when reproduction occurred. There are two general ways to describe reproduction in these models. The birth flow assumption means that reproduction occurs continuously between \\(t\\) and \\(t+1\\). In contrast, the birth pulse assumption means that reproduction occurs in a single pulse at some point between \\(t\\) and \\(t+1\\). In temperate environments the birth pulse assumption often works very well. Many species give birth in distinct seasons much shorter than the annual cycle. Regardless of which way we describe reproduction, individuals that are between age \\(i-1\\) and \\(i\\) are in age class \\(i\\) (Figure 7.1). In a birth pulse model, all individuals have the same birthday (Figure 7.2). We won’t consider the birth flow assumption further in this textbook. If this assumption seems more relevant to your system, you should consult Caswell (2001) — the authoritative reference on the construction and analysis of structured population models. Figure 7.1: The relationship between age classes and ages in an age structured model. The vertical bars represent birth days. Figure 7.2: In a birth pulse model, everyone has the same birthday, and all reproduction occurs on the birthday. The next step is to decide when the census occurs relative to the birth pulse. This is often determined by how data on population size are being collected. For example, if the model is constructed from age at harvest data for white tail deer, the census (hunting season) occurs about 6 months after the birth pulse. If you are counting male and female prairie chickens on leks, then the census is taking place immediately prior to the pulse of reproduction (egg laying, incubation hatching and fledging). The most common assumptions are either immediately before or after the birth pulse. These are referred to as “pre-breeding” or “post-breeding” census models (Figure 7.3). Figure 7.3: In a post-breeding census model, the state of the population is captured right after breeding. In a pre-breeding census model, the state of the population is captured right before breeding occurs. Now we are ready to answer the question posed above. In a pre-breeding census model \\(n_{1,t}\\) represents individuals that were born in the previous birth pulse, one year previously. That is, this is the number of individuals about to have their first birthdays. Consequently, \\(n_{2,t}\\) represents those individuals that are about to have their second birthdays, and so on. In a post-breeding census model \\(n_{1,t}\\) represents individuals that have just been born, while later age classes \\(i\\) are individuals immediately following their \\((i-1)^{th}\\) birthdays. Age class 2 individuals have just had their first birthday. This may seem very pedantic, but these choices actually matter alot when translating biological data on birth and death rates into an age-structured population model. Projecting an age structured population I will first describe how to calculate the population next year from this years population for a simplified model with two age classes. Then I will show how to use matrix arithmetic to do this easily for a model with any number of age classes. Imagine a bird something like a prairie chicken. I census these birds while they are lekking, prior to when the hens lay eggs and produce the next generation. Then birds in the first age class are last year’s fledglings that survived for an entire year to their first birthday. This is a pre-breeding census model. Birds in the second age class come from two sources. They are last years age class 1 birds that have survived to reach their second birthday. Also, birds that were in the 2nd age class last year may have survived to reach their 3rd or later birthdays. Therefore \\[\\begin{align} \\tag{7.1} n_{1,t+1} &amp; = n_{1,t} F_1 S_0 + n_{2,t} F_2 S_0 \\\\ \\tag{7.2} n_{2,t+1} &amp; = n_{1,t}S_1 + n_{2,t}S_2 \\end{align}\\] In these equations \\(S_i\\) is the survival probability from age \\(i\\) to age \\(i+1\\), and \\(F\\) is the number of individuals born to an individual of age class 2. So \\(S_1\\) is the probability of surviving from age 1, in the census right before the first birthday, to age 2, in the census right before the second birthday. The first equation (7.1) says that the number of age class one individuals in year \\(t+1\\) is the sum of two terms. The first term is the product of the number of age class 1 individuals in year \\(t\\), the number of offspring produced by each individual, and the probability that new borns survive to their first birthday. The second term is the same, representing the reproductive contribution from age class two individuals. The second equation (7.2) says that the number of age class two individuals in year \\(t+1\\) is the sum of age class one and age class two individuals that survived from \\(t\\) to \\(t+1\\). This life history can be represented graphically by putting the connections between the states as arrows (Figure 7.4). Figure 7.4: Life history diagram of a Pseudo-chicken. I’ll start by filling in some numbers for my life history parameters. \\(F_i\\) is the number of fledglings produced per individual of age \\(i\\). Hens lay 15 eggs, but the probability of each egg surviving to fledge is only 25%, and half of the eggs are males. So I can set \\(F = \\frac{15 \\cdot 0.25}{2} = 1.88\\). Dividing by 2 is necessary because only the females lay eggs, but assumes that the sex ratio is equal and males are not limiting reproduction. Just for simplicity, we also assume that age 1 and age 2+ females lay the same number of eggs. Survival from fledging to a bird’s \\(1^{st}\\) birthday, \\(S_0\\), is one of the most poorly measured life history parameters ever. It is very difficult to estimate, especially for migratory birds. For illustration purposes I’ll set \\(S_0 = 0.25\\), but you should realize how much that’s a shot in the dark. Survival of adult birds is easier to estimate, but I’ll just set \\(S_1 = S_2 = 0.5\\) for now. If I have a population of 100 age 1 birds at time \\(t\\) then \\[\\begin{align*} n_{1,t+1} &amp; = 100 \\cdot 1.88 \\cdot 0.25 + 0 \\cdot 1.88 \\cdot 0.25 = 47\\\\ n_{2,t+1} &amp; = 100 \\cdot 0.5 + 0 \\cdot 0.5 = 50\\,. \\end{align*}\\] So my total population at time \\(t+1\\) is 97. Repeating the process \\[\\begin{align*} n_{1,t+2} &amp; = 47 \\cdot 1.88 \\cdot 0.25 + 50 \\cdot 1.88 \\cdot 0.25 = 55\\\\ n_{2,t+2} &amp; = 47 \\cdot 0.5 + 50 \\cdot 0.5 = 49\\,. \\end{align*}\\] and now my total population is 94. I could keep going with these equations, but the math is getting tedious. These equations can be readily automated in a spreadsheet, but there is a better way that leads to greater insights into the dynamics of structured populations. 7.2 Population Projection Matrices Equations (7.1) and (7.2) can be written down as the matrix product of a vector and a square matrix \\[\\begin{equation} \\mathbf{N_{t+1}} = \\begin{bmatrix} n_{1,t+1} \\\\ n_{2,t+1} \\end{bmatrix} = \\begin{bmatrix} F_1S_0 &amp; F_2S_0 \\\\ S_1 &amp; S_2 \\end{bmatrix} \\begin{bmatrix} n_{1,t} \\\\ n_{2,t} \\end{bmatrix} = \\mathbf{A N_{t}}\\,. \\tag{7.3} \\end{equation}\\] The matrix \\(\\mathbf{A}\\) is called a population projection matrix. It is always square, with as many rows as there are age classes in the population. The vector \\(\\mathbf{N_t}\\) represents the state of the population at time \\(t\\), and has one entry for each age class. You could be forgiven for wondering how (7.3) is an improvement over (7.1) and (7.2). It turns out that there are many calculations that can be done with \\(\\mathbf{A}\\) that allow us to understand the dynamics of any population. Growing or Declining? Take a careful look at the elements of the matrix \\(\\mathbf{A}\\). None of them depend on the size of the population. Therefore this simple population model is density independent, which means that it will either grow or decay without limit. One way to see which of these will happen is to simply repeat the matrix multiplication many times, and see what happens (Figure 7.5). In this case, the reintroduced population declines steadily. In fact, it looks like the total population size is decreasing exponentially at a constant rate. \\(N_2 = 97\\) while \\(N_3 = 94.1\\), so the change is \\(94.1/97 = 0.97\\). We get the same value comparing \\(N_{10} = 76\\) with \\(N_{11} = 73.4\\). It is as if our population is decreasing geometrically, the same as an unstructured population. Moreover, the proportion of age class 1 and 2 individuals in the population is constant too, after the blip caused by starting with everyone in age class 1. So in effect I would have exactly the same prediction from a simple geometric model with \\(\\lambda = 0.97\\). Figure 7.5: Projecting the pseudo-chicken population forwards in time. In this first example, the life history parameters are identical for both age classes. But what happens if older chickens reproduce better? Experience counts for birds too — older hens have more experience in where to place nests, often nest earlier and so forth. Thus it is entirely reasonable that \\(F_2 &gt; F_1\\), so lets set it to 2.5 and see if that makes a difference (Figure 7.6). And it does! Now the population is increasing, but only after year 3. In the first year the total population size still shrinks. This makes sense, because I started with all age class 1 birds and their fertility is still the same. Once there are some age class 2 birds, in the second year, the population begins to grow because of the higher fertility of older birds. The rate of growth still looks pretty constant at \\(~1.04\\), and after the 4th year the proportion of age class 2 birds is again constant. So I’m back to a geometric model, but it took a bit longer to get there. Figure 7.6: Projecting the pseudo-chicken population forwards in time with a higher Fertility for age class 2. It turns out that for any particular population projection matrix, there is a specific distribution of individuals among age classes that make the matrix behave as if it were a scalar multiplier. This is actually true of any square matrix with non-negative entries. In addition, no matter what age distribution of individuals I start with, the age distribution converges towards this special distribution. Depending on the particular structure of the matrix and the starting distribution of individuals, this convergence can be rapid or it can be slow, but it always happens. This special age distribution is called the “stable age distribution”. It is important to recognize that the proportion of individuals in each age class is stable. The number of individuals is still changing. This makes the stable age distribution different from the stable points found in the Logistic population growth models. When the population is at the stable age distribution, it will grow (or decay) geometrically at a constant rate. The symbol for that growth rate is typically \\(\\lambda\\). When \\(\\lambda = 1\\) the population is neither growing nor declining. It is possible to find the value of \\(\\lambda\\) and the stable age distribution without multiplying the population out until it has stabilized. When the population is at the stable age distribution, then \\[\\begin{equation*} \\mathbf{Av} = \\lambda\\mathbf{v}. \\end{equation*}\\] where \\(\\mathbf{v}\\) is a vector. This equation is called the “characteristic equation” of a square matrix. The values of \\(\\lambda\\) and \\(\\mathbf{v}\\) that make this equation true are candidates for the population growth rate and stable age distribution. There will be as many solutions as there are rows in the matrix. The values of \\(\\lambda\\) are called “eigenvalues” of the matrix, and the corresponding vectors are “eigenvectors”. While the math to find these values is not easy (use a computer!), once you have these solutions the only one we want is the largest real eigenvalue. That will be the growth rate of the population when it is at the stable age distribution, and the stable age distribution is given by the matching eigenvector. I will never expect you to calculate the eigenvalue and eigenvector. You should know that the population growth rate for a structured population at the stable age distribution is the leading eigenvalue of the projection matrix the stable age distribution is predicted by the right eigenvector of the leading eigenvalue. You can always find these values using the multiplication method used in Figures 7.5 and 7.6 in a spreadsheet. 7.3 A more complex example This next example shows how to build up a matrix from data on the number of individuals of each age in a population over a period of years. The data are from a study of great tits (Parus major) in the United Kingdom (Table 7.1). If you sum up the # of birds columns, that is the total population size in each year. Average reproductive performance also varies from year to year. This variation is the result of both environmental variation between years as well as demographic stochasticity. In addition, the reproductive output of the age 1 birds is a bit lower than that of older birds. There might also be a tiny hint of “senesence”, or aging, as the oldest age classes also have lower reproductive output. Survival from one age class to the next can be worked out by working diagonally across the table. For example, birds that are 2 years old in 1962 were 1 year olds in 1961. Therefore the survival probability is \\(43/128 = 0.34\\). The corresponding probability from 1962 to 1963 is \\(33/54 = 0.61\\), much higher than the survival rate from the previous year. ## Warning: Duplicated column names deduplicated: &#39; # of birds &#39; =&gt; &#39; # of ## birds _1&#39; [4], &#39; Mean Clutch &#39; =&gt; &#39; Mean Clutch _1&#39; [5], &#39; # of birds &#39; =&gt; ## &#39; # of birds _2&#39; [6] Table 7.1: Age structure and reproductive performance of great tits (Parus major) in Wytham Wood. Age # of birds Mean Clutch # of birds _1 Mean Clutch _1 # of birds _2 Mean Clutch 1 128 7.7 54 8.5 54 9.4 2 18 8.5 43 9 33 10 3 14 8.3 12 8.8 29 9.7 4 5 8.2 9 9.7 5 1 8 2 9.5 6 1 9 It is possible to work out the Fertility and Survival entries for a projection matrix directly from this type of data. However, if we first convert this data into a “life table” format, there are straightforward formulas to get the matrix entries. Life tables are quite commonly reported, especially in the older literature, so it is useful to know where they come from, and how to get from a life table to a projection matrix. Life tables come in two flavors: static and cohort. A static life table represents a snapshot of a population in a single year. There are two columns, the survivorship schedule, \\(l(x)\\), and the maternity schedule, \\(m(x)\\). The survivorship schedule shows the number of individuals alive on their \\(i^{th}\\) birthday, assuming that everyone is alive on their birthday. This column is often standardized by the number of age 0 individuals so that \\(l(0)=1.0\\). The maternity schedule is the expected # of age 0 individuals produced per individual of age \\(x\\). The columns of Table 7.1 represent 3 different static life tables, with the number of age 0 individuals missing. If we estimate the number of age 0 individuals in 1962 and 1963 as the total egg production in the previous year, then we could get two complete static life tables from this data (Table 7.2). One thing you might notice from this table is that there are an awful lot of age 0 birds that never make it to their \\(1^{st}\\) birthdays! Table 7.2: Static life tables for great tits (Parus major) in Wytham Wood. Survivorship schedule is not standardized. Age l(x) m(x) l(x) m(x) 1962 1963 0 627 0.0 500 0.0 1 54 8.5 54 9.4 2 43 9 33 10 3 12 8.8 29 9.7 4 5 8.2 9 9.7 5 1 8 2 9.5 6 1 9 A static life table can be estimated from a single year’s worth of age data. The biggest problem with them is that the individuals in age 1 are not the survivors of the individuals in age 0 — they are from the previous year. If there is no environmental stochasticity in survivorship then a static life table is an unbiased estimate of the underlying life history. That’s not usually the case, which is why cohort life tables are preferred if multiple years of data are available. The survivorship and maternity schedules in a cohort life table are built up by following cohorts. The 1961 age 0 cohort is age 1 in 1962, and age 3 in 1963. For example, \\(l(2)\\) is the number of age 2 individuals divided by the number of age 0 individuals two years before. To get an estimate using the 1962 birds I have to make some assumptions about how many age 0 birds there were in 1960. As long as the population has not been changing in size, a plausible number is the average of age 0 birds in 1962 and 1963, or \\(l(0) = 564\\). Therefore \\(l_{1963}(2) = 33/564 = 0.059\\), \\(l_{1962}(2) = 43/564 = 0.076\\), and \\(l_{1961}(2) = 18/564 = 0.032\\). Therefore we can set \\(\\overline{l(2)} = 0.056\\), the arithmetic average. The only entry in the cohort table where this approximate number of age 0 individuals is not needed is the age 1 individuals for 1963. The result of repeating all those calculations for all ages is shown in Table 7.3. It is also possible to estimate variances for these estimates because there are multiple years of data. It is beyond the scope of this textbook to get into those calculations. Table 7.3: Cohort life tables for great tits (Parus major) in Wytham Wood. All values are averaged across 1961 to 1963, and assume that the number of age 0 individuals in previous years is 564. Maternity schedules are offspring per adult, or females per female. Age l(x) m(x) 1 1.00 0.0 2 0.14 4.3 3 0.06 4.6 4 0.03 4.5 5 0.01 4.5 6 0.00 4.4 7 0.00 4.5 There are quite a few calculations that can be done directly from the lifetable, but all the same numbers can be more easily calculated from a projection matrix. In addition the projection matrix can be used to project the population structure forward through time, which cannot be done with the life table. Once I have \\(l(x)\\) and \\(m(x)\\) it is easy to get the matrix entries with two simple formulas. Well, actually 4, because the formulas for a post-breeding census matrix differ slightly from a pre-breeding census matrix. For great tits, a post breeding matrix has age class 1 individuals counted right after breeding, as fledglings. Note that in the life table I talk about age, whereas a projection matrix uses age classes. This means that an age class 2 individual has just had their 1st birthday, an age class 3 individual has just had their 2nd birthday, and so on. The first thing I’ll do is write down the formula for the survival probabilities from age class \\(i\\) to \\(i+1\\), \\(S_i\\): \\[ S_i = \\frac{l(i)}{l(i-1)}. \\] These entries go on the 1st subdiagonal of the matrix. So \\(S_1 = l(1) / l(0) = 0.14 / 1 = 0.14\\), and \\(S_2 = l(2)/l(1) = 0.056/0.14 = 0.4\\). Next I write down the formula for the fertilities, the 1st row of the matrix. Each of these entries says, for each individual in age class \\(i\\) at the census, how many age class 1 individuals are present at the next census? Two things have to happen, first the individual has to survive to age class \\(i+1\\), and then they give birth to some number of babies. Therefore \\[ F_i = S_i \\cdot m(i) \\] where \\(F_i\\) is the fertility of age class \\(i\\), \\(S_i\\) is the probability of surviving from age class \\(i\\) to \\(i+1\\), and \\(m(i+1)\\) is the maternity schedule for age \\(i+1\\). So \\(F_1 = S_1 m(1) = 0.14 \\cdot 4.3 = 0.6\\), and \\(F_2 = S_2 m(2) = 0.4 \\cdot 4.6 = 1.84\\). It is very tempting to think of the fertility entries as the number of babies per individual. This is wrong. The fertility always includes the survivorship of another age class. If you are looking at a matrix and you want to know how many babies are produced by that age class, you need to divide the fertility by the appropriate survivorship. Now I can write down the whole matrix \\[ \\mathbf{A}_{post} =\\begin{bmatrix} 0.58 &amp; 1.87 &amp; 2.61 &amp; 1.71 &amp; 0.94 &amp; 3.00 \\\\ 0.14 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.41 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.00 &amp; 0.59 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.38 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.21 &amp; 0.00 \\\\ \\end{bmatrix} \\] This matrix assumes that survivorship from age class 6 to older age classes is zero. This form of the matrix is the classic age structured model called a “Leslie matrix” after the biologist Patrick Leslie who first realized that life tables could be represented using matrices in the 1940’s. The leading eigenvalue of this matrix is \\(\\lambda = 1.03\\), which means the population will increase at about 3% per year. One issue with this matrix is the Fertility of age class 6 is higher than any of the others. This really seems at odds with the otherwise clear pattern of senesence. The maternity schedule entry \\(m(6) = 4.5\\), which is only slightly greater than \\(m(5)\\). However, the survivorship from age class 6 to 7 is 0.67, which is much greater than the previous survivorship of 0.21. Note that \\(S_6\\) is not shown in the matrix. It would be in the 7th row, 6th column, but there are only 6 rows. This estimate is based on a single individual making it to age 6 in one year. There is alot of estimation error in that rate. If I replace \\(S_6\\) with the same value as \\(S_5\\), the fertility for age class 6 drops to 0.964, which is more consistent with the value for age class 5 and seems more reasonable. With this change the leading eigenvalue is \\(\\lambda = 1.03\\). To two decimal places this is unchanged from before. What gives? I reduced the fertility of age class 6 by 2/3, and the growth rate of the population barely budged. One reason for this can be seen from the survival schedule in Table 7.3. Less than 1% of individuals in the population will survive to age class 6. Therefore changes at that end of the matrix will have relatively little effect on the population as a whole. In the next section I’ll introduce some calculations that can tell us directly from the matrix how much the growth rate will respond to changes in a particular entry. Before that, I need to give you the equations for a pre-breeding census matrix. In a pre breeding census matrix age class 1 individuals are counted right before their first birthdays. Similarly, an age class 2 individual is about to have their 2nd birthday, an age class 3 individual is about tohave their 3rd birthday, and so on. The first thing I’ll do is write down the formula for the survival probabilities from age class \\(i\\) to \\(i+1\\), \\(S_i\\): \\[ S_i = \\frac{l(i+1)}{l(i)}. \\] So \\(S_1 = l(2) / l(1) = 0.056/0.14 = 0.4\\), and \\(S_2 = l(3)/l(2) = 0.0325/0.0556 = 0.59\\). These are just like the survivorships from a post-breeding census, but shifted over by one age class. Next I write down the formula for the fertilities, the 1st row of the matrix. The order of events is different for a pre-breeding census. First the individual gives birth to some number of babies. Then, those babies have to survive until just before their first birthday to be counted in the next census. Therefore \\[ F_i = l(1) \\cdot m(i)\\,, \\] because the probability of surviving to the first birthday is the survivorship schedule for age 1. So \\(F_1 = l(1) m(1) = 0.14 \\cdot 4.3 = 0.58\\), and \\(F_2 = l(1) m(2) = 0.14 \\cdot 4.6 = 0.62\\). The whole matrix is \\[ \\mathbf{A}_{pre} =\\begin{bmatrix} 0.58 &amp; 0.62 &amp; 0.61 &amp; 0.61 &amp; 0.60 &amp; 0.61 \\\\ 0.41 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.59 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.00 &amp; 0.38 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.21 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.67 &amp; 0.00 \\\\ \\end{bmatrix}\\,. \\] The fertilities in the top row now look quite different. For one thing, they are much more constant across age classes, as well as smaller for age classes 2 and up. The problematic survival probability is now visible down in the bottom right hand corner. The leading eigenvalue of this matrix is \\(\\lambda = 1.03\\). Even though the matrix looks very different, the predicted growth rate is pretty much identical. For the most part, it doesn’t matter which way the matrix is parameterized. The choice is usually made based on the type of data that is available. Now I’ve captured 100 fledglings and reared them up to their first birthday, and I want to release them into a forest that lost all it’s tits in a fire. I’ll release them right at the beginning of the breeding season, so that they can reproduce right away. I’m going to use the pre-breeding census matrix to project this population forward one year. All the individuals are starting in age class 1. \\[ \\mathbf{N}_{t+1} = \\begin{bmatrix} 58 \\\\ 41 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix} = \\mathbf{A}_{pre} \\begin{bmatrix} 100 \\\\ 0\\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix} \\] So after one year I expect to have 41 age class 2 individuals (the survivors of the original cohort), and 58 age class 1 individuals (the survivors from the fledglings in the first breeding season). The total number is 99, slightly less than I started with. The reason the population is not growing as predicted is that it is not yet at the stable age distribution. One more year: \\[ \\mathbf{N}_{t+2} = \\begin{bmatrix} 59 \\\\ 24 \\\\ 24 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix} = \\mathbf{A}_{pre} \\begin{bmatrix} 58 \\\\ 41 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix} \\] Now I have individuals in 3 age classes, and 107 total individuals! That growth rate is much higher than the 3% I’m expecting. If I keep doing this as equations it will take up alot of paper, so Figure 7.7 projects the total population over 10 years. The population does grow continuously past the 3rd year, but the rate of growth is not constant until year 7 or so. These transient fluctuations in the growth rate have real consequences. After 10 years the expected size of the population is 3 individuals larger than expected under an assumption of a stable age distribution. This phenomenon is called “transient amplification”. It is also possible to get “transient attenuation”, which is where the fluctuations in growth rates lead to a smaller population size. Figure 7.7: Total population of great tits projected over 10 years. The thin dashed line shows the expected growth for a population starting at the stable age distribution. How many age classes? In the great tit example I used 6 age classes because I had individuals up to 6 years old. However in other cases I might have individuals that are much older. Or, as in the great tit example, there are very few individuals in the older age classes, and the quality of the survival and reproduction estimates goes down. The real question is, how many age classes should I model? In the classic Leslie matrix, the survival of individuals from the last age class to the future is assumed to be zero. This isn’t necessary. Instead I could treat the last age class as a “lumped” age class that includes all individuals of that age and older. This is what I did with the pseudo-chickens in the first example I presented. The survival element in the bottom right hand corner of the matrix was a positive value. If I assume that individuals in all older age classes have the same survival probability, I can do the same with the great tit pre-breeding census matrix which gives me \\[ \\mathbf{A}_{pre} = \\begin{bmatrix} 0.58 &amp; 0.62 &amp; 0.61 &amp; 0.61 &amp; 0.60 &amp; 0.61 \\\\ 0.41 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.59 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.00 &amp; 0.38 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.21 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.67 &amp; 0.67 \\\\ \\end{bmatrix} \\] which has a leading eigenvalue of \\(\\lambda = 1.04\\), slightly larger than before. This makes sense, because instead of assuming that everyone dies before their 7th birthday, I’m allowing them to live on as long as they survive, and the survival rate I’m using is quite high. Introducing a lumped age class can really simplify things. Age classes 5 and 6+ are quite similar — same survival rate and nearly the same reproductive rate. This suggests that I could collapse the matrix back to 5 age classes like this: \\[ \\mathbf{A}_{pre} = \\begin{bmatrix} 0.58 &amp; 0.62 &amp; 0.61 &amp; 0.61 &amp; 0.60\\\\ 0.41 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00\\\\ 0.00 &amp; 0.59 &amp; 0.00 &amp; 0.00 &amp; 0.00\\\\ 0.00 &amp; 0.00 &amp; 0.38 &amp; 0.00 &amp; 0.00\\\\ 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.21 &amp; 0.67\\\\ \\end{bmatrix} \\] which has a leading eigenvalue of \\(\\lambda = 1.04\\). This is pretty much identical to the rate for the 6 class matrix. Previously I suggested that the survival probability for the 4th age class was a better choice for older age classes, and in that case I can further trim the matrix back to a 4 class matrix \\[ \\mathbf{A}_{pre} = \\begin{bmatrix} 0.58 &amp; 0.62 &amp; 0.61 &amp; 0.61 \\\\ 0.41 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.59 &amp; 0.00 &amp; 0.00 \\\\ 0.00 &amp; 0.00 &amp; 0.38 &amp; 0.21 \\\\ \\end{bmatrix} \\] which has a leading eigenvalue of \\(\\lambda = 1.03\\). This is now quite a bit lower than the larger matrices, but given the large drop in the survival probability of the older age classes (from 0.67 to 0.21), that is not surprising. In the previous section I reduced the survival probability for age class 6 in the post breeding matrix, and obtained nearly the same population growth rate. The lumped upper age class is a good way to simplify things. In general, adding age classes to a matrix will increase \\(\\lambda\\), but only up to a point. Eventually the proportion of individuals reaching the new age class becomes very small, and consequently the population growth rate stops increasing. 7.4 Sensitivity and elasticity I want to return to the problem of how much the population growth rate changes in response to a change in one of the matrix elements. Mathematically, I want to calculate \\[ s_{i,j} = \\frac{\\partial \\lambda}{\\partial a_{i,j}} \\approx \\frac{\\lambda - \\lambda^\\prime}{a_{i,j} - a_{i,j}^\\prime} \\] where \\(s_{i,j}\\) is the sensitivity of the population growth rate \\(\\lambda\\) to a change in the matrix element \\(a_{i,j}\\). If you have software that can calculate eigenvalues and eigenvectors the sensitivity can be calculated directly. However, it can be done in a spreadsheet. First calculate \\(\\lambda\\) using the multiplication method. Then change one matrix entry by a small amount (say 1%) to a different value \\(a_{i,j}^\\prime\\), and calculate \\(\\lambda^\\prime\\). Then use the approximate formula. It will quickly become tedious for a large matrix. For this text, it is enough to know how to interpret the values I provide for you. The matrix of sensitivity values for the great tit matrix is in Table 7.4. Table 7.4: Sensitivity matrix for the pre-breeding census model of great tits. Sensitivities for zero entries in matrix are fixed at zero. 1 2 3 4 5 6 0.58 0.23 0.13 0.05 0.01 0.01 0.64 0.00 0.00 0.00 0.00 0.00 0.00 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 So how do you read this thing? Consider the value in the 1st row, 6th column. This number is the sensitivity for the fertility of age class 6 that I messed around with above. This number means that if I change the fertility by 1, the growth rate will change by 0.01. The number is positive, so an increase in fertility yields an increase in population growth rate. Conversely, a decrease in the fertility will decrease the population growth rate. Compare that with the sensitivity in the 2nd row, 1st column. This is the response of the population growth rate to changes in the survival probability from age class 1 to age class 2. The value of 0.64 is many times larger, and in fact is the biggest value in the whole table. Notice that the values decrease quite rapidly as individuals age. This is due to the same effect pointed out before. Fewer and fewer individuals are making it to those older age classes, and therefore the survival and fertility values have progressively less effect on population growth. Matrix entries that are not biologically reasonable, like \\(a_{6,1}\\) the 6th row and 1st column, also have sensitivity values, but typically these are fixed at zero. One issue with sensitivity values arises when trying to compare values between matrix entries that have very different magnitudes. For example, the survivorship of age class 4 is only 0.21, which is 1/3 the magnitude of the fertility values. A change of 0.05 is a 25% change to the survivorship, but only a 10% change to the fertility. One way to fix this issue is to convert the sensitivity values to a relative scale; these are called elasticities. The formula is pretty easy: \\[ e_{i,j} = s_{i,j}\\frac{a_{i,j}}{\\lambda}\\,. \\] The elasticities are shown in Table 7.5. Now all the values are on the same scale. A change of 1 % in the survival probability from age class 1 to age class 2 (\\(a_{2,1}\\)) yields a 0.25% change in the population growth rate. Notice that the matrix entry that has the largest value is now different — the elasticity of age class 1 fertility is greater than the elasticity of age class 1 survival. Otherwise the conclusions from sensitivity are unchanged. Survival and fertility of older individuals has less impact on population growth than that of younger individuals. Table 7.5: Elasticity matrix for the pre-breeding census model of great tits. 1 2 3 4 5 6 0.327 0.139 0.077 0.028 0.006 0.004 0.253 0.000 0.000 0.000 0.000 0.000 0.000 0.115 0.000 0.000 0.000 0.000 0.000 0.000 0.038 0.000 0.000 0.000 0.000 0.000 0.000 0.010 0.000 0.000 0.000 0.000 0.000 0.000 0.004 0.000 The sensitivity and elasticity values can be used in a couple of different ways. First, imagine you have two different management actions that affect two different matrix elements by 1%, and the cost of the two actions is the same. In this case you should chose the action with the greater elasticity, because it will have a greater effect on the population growth rate for the same cost. The sensitivity is useful in choosing between management actions even if the costs and magnitudes of the effects are different; the math just gets more complicated. Second, imagine that you have a limited budget for research, and you must decide whether to spend the money on estimating the reproductive success or survival of adults. Improving estimates of the parameter with the greater sensitivity will (usually) reduce the variance in estimated population growth rates the most. 7.5 Stage or size structured populations Populations can also be structured by size or stage. This sort of structure is common with fish and plant populations, where age is often harder to measure than size. Fish and plants often have indeterminate growth, that is, they continue growing throughout their lives. In addition, size often affects both survival and fecundity of individuals, so size is a better way to break up such populations. Even if we switch to size or stage as the defining characteristic of a structured population, the model still has a fixed time step over which the population is projected. In all the examples in this course that time step is one year. If each column and row in the matrix represents a stage in which an individual animal can remain for longer than a single time step, then we need to change the definition of some of the entries in our matrix. In particular, the entries just below the main diagonal of the matrix are now the product of two processes: survival and growth. If the probability of growth from one stage to the next is less than 1, then we also need to have an entry on the main diagonal that presents the event of surviving and NOT growing to the next stage. Perhaps the most highly cited stage structured model is of Loggerhead turtles (Crouse, Crowder, and Caswell 1987). Crouse et al. divided the population into 7 biologically relevant classes (Table 7.6). Some of the stages are easy to identify in the field; hatchlings are very different from breeding adults. It is much harder to sort out juveniles. These animals are rarely observed because they spend all their time at sea, and may move around a great deal. Tagging individuals to understand growth rates is problematic because the carapaces and flippers tend not to hold tags very well. The breeding adults are also broken into three distinct groups to accommodate differences in the reproductive output among first year breeders and mature breeders. One of the big uncertainties in sea turtle dynamics is understanding the frequency that adults breed. Do they lay eggs every year? Every other year? Individuals that attempt to breed every year (remigrants) produce many fewer eggs than individuals that wait longer. Table 7.6: Stages used in the Crouse et al. model of loggerhead sea turtles Stage number Class Size (cm) Approximate ages (yr) 1 eggs, hatchlings &lt; 10 &lt; 1 2 small juveniles 10.1-58.0 1-7 3 large juveniles 58.1-80.0 8-15 4 subadults 80.1-87.0 16-21 5 novice breeders &gt; 87.0 22 6 1st-yr remigrants &gt; 87.0 23 7 mature breeders &gt; 87.0 24-54 Table 7.7: Stage based projection matrix for Loggerhead sea turtles. 1 2 3 4 5 6 7 0.000 0.000 0.000 0.000 127.000 4.000 80.000 0.675 0.737 0.000 0.000 0.000 0.000 0.000 0.000 0.049 0.661 0.000 0.000 0.000 0.000 0.000 0.000 0.015 0.691 0.000 0.000 0.000 0.000 0.000 0.000 0.052 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.809 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.809 0.809 The turtle matrix (Table 7.7) demonstrates several features of a structured population. The top row represents the contribution of individuals in each stage to the first stage (eggs and hatchlings) at the next census point. The values are zero for the first four stages because these stages do not reproduce. Transitions between stages 5, 6 and 7 look similar to an age structured matrix because stages 5 and 6 only last 1 year. Similarly, individuals starting as hatchlings are automatically promoted to small juveniles in a single year, so the entry in the first row and column is zero. 67% of hatchlings will survive to be small juveniles in the next year. The small juvenile, large juvenile and sub-adult classes are where things look different. The entry in the 3rd row, 2nd column (0.0486) means that \\(\\approx\\) 5 % of small juveniles will both survive the year and grow to be large juveniles. The entry right above that, 2nd row, 2nd column (0.737) means that \\(\\approx\\) 74% of small juveniles will survive the year but remain small juveniles the next year. If we add these two values together we get 0.786 which is the annual probability of survival for a small juvenile. We can do all the same sorts of calculations with this matrix that we did with the age structured matrices. The leading eigenvalue of this matrix is 0.945, which means that the population is declining at about 5% per year. The stable stage distribution of the population (Figure 7.8) is very skewed towards the first two stages. The breeding population represents much less than 1% of the total population even though that stage lasts the longest. The high mortality during the juvenile stages means that very few individuals survive to become breeders. Figure 7.8: Stable stage distribution for the loggerhead turtle population projection matrix There are two management interventions that are used for sea turtle conservation. The first is “headstarting”, the practice of monitoring clutches on nesting beaches, and then helping hatchlings to get to the sea without being eaten. This will increase the Fertility entries in the first row, because mortality on the beaches is included in the estimates of Fertility. The other action is to equip trawlers with “Turtle Excluder Devices”. These additions to the throat of a trawl net catch turtles before they are caught in the cod end of the net, and give them a hatch to escape out of. A TED will improve survival, particularly of larger turtles that are more likely to be caught on the TED’s grid. Headstarting is easy, in part because it is highly visible and gives everyone warm fuzzy feelings. The effects of TEDs are largely invisible, and require the consent of fishing communities to implement. Which one should we use? Consider the elasticity matrix first (Figure 7.9). Headstarting will increase the entries in the top row, and adding those all together gets a total of a 5% change in \\(\\lambda\\) for a 1% increase in fertility. Compare that with the sum of the elasticities for increasing survival of large juveniles and up: 0.666, or more than 12 times larger increase for the same relative improvement. This suggests that improvements in survival of larger stages will be more effective at improving population growth rates than increases in fertility. Figure 7.9: Elasticity matrix for loggerhead turtle populations. Each entry represents the relative change in population growth rate for a 1% change in that entry. Elasticities are useful, but really only evaluate small changes in matrix entries. Given the uncertainty in reproduction it is plausible that we could double the Fertilities, but not much more than that. Doubling the fertility entries gives us \\(\\lambda = 0.9814796\\), which is still less than 1. Headstarting may slow the rate of decline in the population, but it cannot plausibly lead to a growing population by itself. In contrast, a modest 10% increase in the survival of large juveniles and up yields \\(\\lambda = 1.0099952\\). TEDs seem like a very good option because they can reverse the declining population trend with even if they produce only modest improvements in survival. These analyses were used to support regulations requiring TEDs in many fisheries around the globe. 7.6 Exercises Figure 7.10: Stage structured pre-breeding projection matrix for Desert Tortoise Gopherus agassizii (Doak, Kareiva, and Klepetka 1994). The color codes highlight different magnitudes of the entries, with red having the largest value. Using the stage structured matrix in Figure 7.10, what does the value in the first row, 6th column mean? The sum of the values in the second column is 0.716. What does this sum represent? What does the value in the 4th row and 4th column represent? A gopher tortoise nest is recovered from a construction site, and 100 hatchlings are reared to their first birthday and then released back into the wild with radio collars. How many are expected to survive the next year as they grow into the Juv1 stage? You have two management options: headstart eggs, increasing fertility of all adult age classes, or predator control which increases survival of yearlings and juveniles. Using the elasticity matrix in Figure 7.11, which management action will have the largest relative effect on population growth rate. The leading eigenvalue of this projection matrix is 0.9580592. Is this population increasing or decreasing, and how fast? Figure 7.11: Elasticity matrix for gopher tortoise References "],
["chap-conservation.html", "Chapter 8 Conservation of small populations 8.1 Minimum viable population size 8.2 Extinction risk and expected minimum population size 8.3 Dealing with habitat fragmentation", " Chapter 8 Conservation of small populations There are three broad categories of goals for population management: harvesting, control, and conservation. This chapter focuses on the third one, conservation of small populations. As before, I want to be able to evaluate management alternatives intended to increase population size with the intent of being efficient about achieving conservation objectives. All the tools developed to this point are still relevant. The biggest difference between conservation and other management goals is that the populations are often very small. This means that in addition to deterministic effects on the life history, there is a much greater need to pay attention to stochastic effects. When there are fewer than 100 individuals in a population, a run of bad luck over a few years can dramatically reduce the size of the population. Populations can be extirpated in one of two ways. As mentioned above, a population that has reached small numbers has a substantial risk of extinction even if the average growth rate of the population is positive, just due to stochastic effects. This is called stochastic extinction. Marc Mangel and Charles Tier pointed this out in their paper “Four facts every conservation biologist should know” (Mangel and Tier 1994). Some authors have described concerns about stochasticity as the “small population paradigm”. At least one scientist described this paradigm as “worrying about the rivets in the wing when the plane is already falling out the sky.” The second way a population can go extinct is deterministically. That is, the life history of the population is such that the average growth rate is negative. Such a population decays exponentially towards zero. This type of extinction is sometimes called a driven extinction. These two categories are not exclusive. A population undergoing a driven extinction also suffers from the effects of stochasticity as it becomes small. Mangel and Tier’s Four Facts The starting point for Mangel and Tier’s facts is a simple population model first presented by Robert MacArthur and Edward Wilson in their classic 1967 book Island Biogeography. A population can grow, on average, exponentially and without bound and still not persist. There is a simple and direct method to compute persistence times that virtually all biologists can use. The shoulder of the Macarthur-Wilson model occurs with other models but disappears when catastrophes are included. Extinction times are approximately exponentially distributed and this means extinctions are likely. 8.1 Minimum viable population size In 1981 Mark Shaffer proposed a “rule of thumb” for the persistence of a population called the Minimum Viable Population Size (MVP). His definition of an MVP was A minimum viable population for any given species in any given habitat is the smallest isolated population having a 99% chance of remaining extant for 1000 years despite the foreseeable effects of demographic, environmental, and genetic stochasticity, and natural catastrophes. That definition sounds fine, but it doesn’t describe how to actually calculate what the MVP size would be. In Chapter 5 I discussed some of the effects of demographic and environmental stochasticity, as well as the effects of natural catastrophes. What about genetic stochasticity? Ian Franklin (1980) suggested that the effective population size needed to be \\(&gt;50\\) to avoid immediate inbreeding effects, and \\(&gt;500\\) to prevent serious loss of genetic variation. These numbers have become entrenched in the conciousness of generations of biologists, but where do they come from? The primary issue with the genetics of small populations has to do with genetic drift, which is variation in the relative frequency of different genotypes in a small population, owing to the chance disappearance of particular genes as individuals die or do not reproduce. (Google!) Genetic drift can lead to the loss of an allele at a particular locus; this leads to a reduction in genetic variation at the locus. This type of loss is more likely when populations are small, and when sex ratios (ratio of males to females) depart from the ideal for a population. Thus genetic drift decreases with increasing population size, and decreases as a population gets closer to the ideal sex ratio. Population geneticists measure genetic variation with a number called Heterozygosity (\\(H\\)). Heterozygosity is the proportion of genetic loci in a population that have more than one allele. If all loci are fixed at a single allele, then there is no genetic variation in a population, and no possibility for natural selection to generate changes. The observed heterozygosity \\(H_o\\) at a target locus in a population of \\(n\\) individuals is given by \\[\\begin{equation} H_o = \\frac{\\sum_{i=1}^{n}{1 \\,\\text{if}\\, a_{i1} \\neq a_{i2}}}{n} \\end{equation}\\] where \\(a_{ij}\\) is the allele on the \\(j^{th}\\) chromosome of the \\(i^{th}\\) individual. In an ideal population it is possible to calculate the expected heterozygosity given the frequencies of the alleles at the target locus is \\[\\begin{equation} H_e = 1 - \\sum_{i=1}^{m}{\\left(f_i\\right)^2} \\end{equation}\\] where there are \\(m\\) alleles at the target locus, and \\(f_i\\) is the relative frequency of each allele. An ideal population is infinite in size, equal in sex ratio and panmictic (everyone has an equal chance of mating with everyone else). Obviously real populations are not ideal, and therefore \\(H_o &lt; H_e\\). Worse, if a population is a long ways from ideal, it is possible to lose heterozygosity over time just due to chance effects. This is the concern with genetic stochasticity. One way to quantify the departure of a real population from the ideal population is to calculate the effective population size. This is an imaginary population with a perfect sex ratio that looses heterozygosity at the same rate as the observed population. This calculation takes into account differences in the contribution of males and females to subsequent generations. It is the harmonic mean of the number of male and female individuals, and as such it is reduced if the sex ratio departs from an ideal of 50:50 (Figure 8.1A). The effective population size is given by \\[\\begin{equation*} N_e = \\frac{4}{\\frac{1}{N_f} + \\frac{1}{N_m}}\\,. \\end{equation*}\\] Figure 8.1: A Effective population size as a fraction of actual population size for different proportions of males in the population. B Change in Heterozygosity as a function of total population size for \\(p_m = 0.5\\) (solid line), an ideal population, and \\(p_m = 0.3\\) (dashed line). The relationship between effective population size and loss of heterozygosity is fairly simple. The expected annual change in heterozygosity is \\[\\begin{equation} \\frac{H_{t+1}}{H_t} = \\Delta H = 1 - \\frac{1}{2N_e}\\,. \\end{equation}\\] So as effective population size increases, the expected ratio of \\(H_{t+1}\\) to \\(H_t\\) approaches 1 (Figure 8.1B). With this equation I can work out what levels of loss Franklin considered “acceptable”. \\(N_e = 50\\) implies \\(\\Delta H = 0.99\\), while \\(N_e = 500\\) implies \\(\\Delta H = 0.999\\). Thus the common rules of thumb imply that rates of heterozygosity loss should not exceed 1% per year or 0.1% per year for long term persistence. It is clear why \\(N_e &gt; 50\\) seems like a good rule of thumb. At smaller population sizes the rate of Heterozygosity loss increases rapidly. However, even at an effective population size of 10 the loss rate is only 5% per year. When the population is that small there are much bigger concerns to worry about! The other thing to notice in Figure 8.1A is that skewing the sex ratio down to 30% males hardly moves the curve at all. That is a large departure in sex ratio, which suggests that it might not be that big of a concern at all. All of these calculations also assume that the ideal sex ratio is 50:50, which isn’t the case for species with mating systems other than purely monogamous. The primary reason to be concerned about a loss of heterozygosity is the potential for “inbreeding depression”. Every species has some alleles for many loci that are either non-functional or outright deadly. Mostly these alleles remain infrequent in a population, because whenever they are expressed they reduce survival, fecundity or both, and thus are strongly selected against. However, if a deleterious allele is recessive, it can persist in a population despite the selection, because it only causes problems when an individual is unlucky enough to get two copies of the allele. In a large population that is unlikely. However as populations shrink, or as heterozygosity decreases, the probability of expressing these deleterious alleles goes up. In very small populations matings between related individuals increases in frequency, and that increases the rate of expression still more. A population experiencing a long term decline begins to lose heterozygosity at an ever greater rate, and inbreeding increases, leading to declines in fertility and survival, which further accelerates the rate of population decline. This positive feedback process has been dubbed an “extinction vortex”. The only instance I am aware of where genetic concerns rose to the level of taking action directly is for the Florida Panther, an endangered subspecies of cougars (Felis concolor). This population occurs in extreme southern Florida, reduced to an area less than 5% of its pre-European range. In the early 1990’s biologists studying the population concluded that inbreeding depression was responsible for reduced fertility of the remaining females. By 1995 the population was reduced to an estimated 20-30 individuals, well within the range of rapid loss of heterozygosity. In 1995 8 female cougars from a closely related population in Texas were released in southern Florida. The hope was that an injection of genetic diversity from these females would improve reproductive success, and reverse the extinction vortex that the population had fallen into. By 2003 the population had approximately tripled to over 80 animals, and the remaining three texas lions were captured and removed from the population. Without alot more work it will be hard to tell whether the increase was due to increased genetic diversity, demographic good luck, or both. Ultimately the population is sharply limited by the available habitat, which remains under extreme pressure from development and increasing human population growth in southern Florida. 8.2 Extinction risk and expected minimum population size When I introduced the concepts of demographic and environmental stochasticity in Chapter 5 I focused on projecting the mean and variance of a population into the future. As time passes, the variance of a population subject to stochasticity also increases. This is why a population that grows on average can still decline — part of the distribution in the future is less than the starting population size. From a conservation standpoint the part of the distribution I worry about the most is the extreme tail. What is the probability that a population starting out at \\(N_0\\) will go extinct? This is exactly what is needed to calculate the MVP defined by Shaffer. A 99% chance of remaining extant is a 1% chance of going extinct. If I ignore catastrophes and genetic stochasticity I can use the same model in (5.4) and (5.5) to work out what the probability of a population not going extinct, ever. If \\(d &gt; b\\) that probability is zero. That is, if average death rates exceed average birth rates the population will go extinct. However, if \\(d &lt; b\\) and the population is not limited by a carrying capacity, then there is a probability that it will persist and that is given by \\[\\begin{equation} p\\left\\{Persistence | N_0\\right\\} = 1 - \\left(\\frac{d}{b}\\right)^{N_0}\\,. \\tag{8.1} \\end{equation}\\] The term \\(\\frac{d}{b}\\) is less than 1 by definition, so as the population size increases this probability gets closer and closer to 1 (Figure 8.2). In fact for very reasonable values of \\(b\\) and \\(d\\) the probability rapidly approaches 1. From this perspective 50 would seem to be a very reasonable MVP. There are a couple of reasons why these curves are too optimistic. These formulas treat population size as continuous (that is, \\(N_0 = 1.2745\\) is reasonable), but populations are made up of individuals. Having discrete individuals creates “lattice effects”, which are very similar to the effects of demographic stochasticity. But far worse the is the consequence of ignoring catastrophes. Figure 8.2: Probability of persistence as a function of initial population size. Solid line - \\(b = 0.4\\) and \\(d = 0.1\\). Dashed line - Catastrophes added on average every 20 years killing an average of 55 individuals. Catastrophic events are characterized by two things: the frequency they occur at and the distribution of the number of individuals killed. The northern bobwhite data in Figure 5.8 provided an example of a catastrophe that occured in 1983. Diseases can also cause catastrophic mortality. Although the true magnitude of the event will likely never be known, the outbreak of Epizootic Hemorrhagic Disease in deer in 2012 was much larger than any observed in past years in Nebraska and neighbouring states. Mangel and Tier (1994) presented theoretical results that showed including catastrophes dramatically reduced the probability of persistence (Dashed line in Figure 8.2). Moreover even much larger populations still have substantial probabilities of going extinct. Similarly, including an upper limit on population size also reduces the probability of persistence. These theoretical results are nice, but only work for very simple population models. Adding complexities like age structure and 2 sexes make the the computations unmanageable. Although Mangel and Tier claimed their method could be used “…by virtually all biologists.” as far as I know it never has been used! It is always possible to construct computer simulations that approximate these probabilities over some fixed period of time. These simulations, often called “Population Viability Analyses” (PVA), have the advantage that as much biological complexity can be included as there is data to support. The disadvantage is that the output is also difficult to summarize effectively. Each different set of life history parameters generates a distribution of population sizes at some future point (Figure 8.3). The red line in the right hand panel assumes \\(r_{max}\\) is larger, so the extinction risk is reduced. A management option that has a cumulative frequency curve farther to the right has a lower risk of extinction. One way to summarize these distributions is to select a particular population size, and then calculate the probability that the simulated population will fall below that level. This critical value is known as a “quasi-extinction threshold”. One side benefit of this approach is that potential (and typically unknown) effects of inbreeding depression can be avoided by selecting a value for quasi-extinction that is above the population size where these effects become large. This is essentially selecting a point on the x-axis of Figure 8.3B, and reading off the proportion of replicate runs that fall below that point. While this gives us a single number to work with, the choice of which value of the population size to designate as “critical” is subjective, and so it is easy to skew results one way or the other by making different choices. If you want a high probability of quasi-extinction select a high value and/or a long time period. If you want a low probability select a low value and/or a short time period. Although Mark Shaffer’s original definition of an MVP used a threshold of 99% persistence over 1000 years, there is in fact little or no standardization of these values. Figure 8.3: Quasiextinction risk at 50 years for a logistic model of discrete individuals with demographic stochasticity. Left hand panel has one line for each replicate to show the distribution of outcomes. Points on each line indicate the smallest population size for each replicate. Parameters for the logistic equation are K = 150, r = 0.05. Right hand panel shows the cumulative distribution of population size at t = 50. Black line: distribution from right hand panel. Red line: K = 150, r = 0.1. Another way to summarize the distribution in Figure 8.3 that is less arbitrary involves calculating the expected minimum population size (McCarthy and Thompson 2006). This is easily calculated for any model. Simply record the smallest population size observed in each run of the model at any point in time (the dots in Figure 8.3A), and take the average of these values. The model run with \\(r_{max} = 0.05\\) has an expected minimum population size of 24.377. Increasing \\(r_{max}\\) to 0.1 increases expected minimum population size to 28.664. This is better than the quasiextinction risk because a population can fall to a low value at any point in the time period, and can often be much lower in the middle of the trajectory than it is at the end. Such population minima represent points of increased risk of extinction whenever they occur. Mick McCarthy and Colin Thompson demonstrated that the expected minimum population size is more sensitive to changes in life history parameters than the probability of extinction or quasiextinction, and this makes it a better metric to use when comparing management options. Although PVA models are increasingly common, they are not without critics. Stephen Beissinger and Michael Westphal (1998) highlighted many of the pitfalls of using them. In particular they argued that estimation errors in model parameters rendered predictions of extinction risk problematic or even useless. It is true that variation in the parameters lead to errors, or perhaps better, variation in the predictions of a model. Despite that, models can still be useful, and in particular, it has been shown that the relative predictions of a PVA model are quite robust to variation in the parameters (McCarthy et al 2003). By relative prediction I mean the rank order of a series of alternative management choices. So if option A is better than option B, then it is almost always better regardless of estimation errors in the parameters. Empirical tests of PVA models in several different circumstances have shown that short term prediction of population size and distribution can be quite good. How well that performance translates into long term performance in the face of changing environments remains an open question. 8.3 Dealing with habitat fragmentation One key assumption I’ve made up to this point in the book is that populations are closed. That is, that there is a single area within which individuals can freely mix. However, one of the most dramatic effects of human development on the landscape is the fragmentation of natural habitats. With few exceptions, human modification of the landscape for human benefit results in isolated patches of natural habitat seperated by areas that are less suitable, or even completely unsuitable, for wildlife. Grazing livestock on rangelands is one of the exceptions. That this fragmentation has consequences for populations was recognized long ago, but it wasn’t until the late 1960’s that theoretical models emerged to predict the effects. Robert MacArthur and Edward Wilson created Island Biogeography theory to explain variation in the number of species found on islands, but early conservation biologists were quick to apply the same ideas to “islands” of habitat in a sea of agriculture. The analogy was crude, and often did not work well. In part this was because Island Biogeography was meant to apply to evolutionary timescales, and conservation biologists were working on recently isolated habitat patches. In addition, conservation biologists were often concerned with single species rather than the ecological communities predicted by Island Biogeography. Two important concepts were highlighted however. First, that the probability of extinction of a species decreases with the area of the island. Second, that the isolation of an island predicts the probability that a species will colonize it. Islands that are farther away from a mainland or other islands will have a lower probability of being reached by dispersing individuals. At about the same time, Richard Levins published a couple of papers describing what he called a “metapopulation” — a population of populations. In the simplest version of the model, each subpopulation in the metapopulation is either occupied by a species or unoccupied. All subpopulations are assumed to be the same size and equally distant from each other. In that case, occupied subpopulations become unoccupied when local extinction occurs at a rate \\(e\\cdot P\\), where \\(P\\) is the proportion of subpopulations that are occupied. The rate constant \\(e\\) is multiplied by the proportion of occupied subpopulations because only these populations can go extinct. Unoccupied subpopulations become occupied when dispersers from other occupied subpopulations reach them and successfully colonize. This occurs at a rate \\(c\\cdot P\\left(1-P\\right)\\). Putting these two processes together I get the Levins metapopulation model for change in the proportion of occupied subpopulations \\[\\begin{equation} \\frac{dP}{dt} = cP\\left(1-P\\right) - eP \\tag{8.2} \\end{equation}\\] As with population models, I want to identify the equilibrium points in this model that occur when the rate of change is zero. When \\(P=1\\) the only thing that can happen is extinction, the rate of change is necessarily less than zero (unless \\(e=0\\)) and so that is not an equilibrium point. If I set (8.2) equal to zero and solve for \\(P\\) I get \\[\\begin{equation} P^* = 1 - \\frac{e}{c} \\end{equation}\\] which says that the equilibrium proportion of occupied habitats is greater than zero as long as \\(e&gt;c\\), because then \\(\\frac{e}{c} &lt; 1\\). An increase in the rate of local extinctions decreases the proportion of occupied patches, and an increase in the colonization rate increases the proportion. Notice the similarity between these conclusions and the conclusions about the extinction from a single population from (8.1). When \\(P=0\\), \\(dp/dt = 0\\) because both colonization and extinction terms are zero. Although this is an equilibrium point, it is not stable because even a small increase in P will lead to a further increase unless \\(e&gt;c\\). That is, if the colonization rate is greater than the extinction rate the proportion of occupied subpopulations will increase. This is a very simple model, and the assumptions are obviously over simplified. However, the model makes one very profound and important prediction for fragmented habitats. Even when everything is going well, the proportion of habitat patches that are occupied by a species is less than one. That is, some otherwise suitable habitat will not be used. This is a very common observation in nature, and the Levins model suggests that it should not be a surprising one. The Levins model and Island Biogeography laid the foundations of modern metapopulation theory. The essential elements of all PVA models of species in fragmented habitats revolve around the ideas of the effects of area on extinction, the effects of isolation on colonization and extinction, and the central importance of patch occupancy. Metapopulation models range from simple to extremely complex. In the following sections I will describe two models that have been applied to addressing simple management questions. 8.3.1 Sumatran tiger management The Sumatran subspecies of tiger (Pantheris tigris sumatrae) is classified as critically endangered by the IUCN. There are only 4 extant subpopulations in the region of the Kerinci Seblat National Park, Sumatra (Figure 8.4). Linkie et al (2006) studied these populations and used a PVA model to calculate the probability of extinction of each in the absence of management over 50 years (a do nothing option), as well as the change in extinction probability for different levels of investment. For the most part the investments represent anti-poaching patrols to prevent illegal harvest of tigers in and around the park. The problem that must be solved is how to allocate poaching patrols among the 4 subpopulations. Should they all be patrolled equally? Put all the eggs in one basket? Allocate effort in proportion to population size? Figure 8.4: Core tiger habitat in the Kerinci Seblat region of Sumatra, with the four different habitat types. Adapted from Linkie et al 2006. Chauvenet et al. (2010) formulated this as a decision theory problem. Using the PrOACT terminology from Chapter 2, the Problem was stated above: allocate a fixed number of poaching patrols among the 4 subpopulations. The Objective that they chose was to minimize the probability that one or more of the subpopulations would go extinct. In the absence of management this is given by \\[\\begin{equation} Pr\\left\\{\\text{one or more subpopulations extinct}\\right\\} = 1 - \\prod_{i=1}^{4}{\\left(1-P_{0i}\\right)} \\tag{8.3} \\end{equation}\\] where \\(P_{0i}\\) is the probability of the \\(i^{th}\\) population going extinct without management over a 50 year period. (8.3) starts with the probability that a subpopulation does not go extinct, \\(1-P_{0i}\\). The probabilty that none of the subpopulations go extinct is simply the product of the four individual subpopulations not going extinct. This assumes that whether or not one of the subpopulations goes extinct is an event that is independent of whether or not the others do. Finally, by the law of total probability (the probability of all events has to add up to one), the probability of one or more subpopulations going extinct is just one minus the probability of none of the populations going extinct. They only had a single objective because they use the total budget available as a constraint on the range of options they could look at. Table 8.1: Alternative budget allocation rules. Recall that \\(\\alpha_2 = 0\\) always. Alternative Values Split budget equally \\(\\alpha_i = 0.33\\) Split budget in proportion to extinction risk \\(\\alpha_1 = \\alpha_3 = 0.23\\), \\(\\alpha_4 = 0.54\\) Put the whole budget in 4 \\(\\alpha_1 = \\alpha_3 = 0\\), \\(\\alpha_4 = 1.0\\) Having set the objective, the next step is to consider the alternatives. As it happens, Core area 2, the largest of the four (see Figure 8.4), has a zero probability of extinction regardless of the degree of poaching. However Linkie et al. only evaluated poaching up to 5 tigers per year, so if poaching was in fact higher then this conclusion wouldn’t hold. However, removing 5 tigers per year from each of the smaller subpopulations leads to a 50 year extinction probability approaching 1. So Chauvenet et al. considered options that divide the anti-poaching budget up between subpopulations 1, 3 and 4. Implementing anti-poaching patrols in an area requires some constant fixed cost regardless of the effort involved, and then some variable cost per unit of effort. So for a total budget \\(b\\) the amount of anti-poaching patrol effort for an area \\(i\\) is \\[\\begin{equation*} A_i = \\frac{\\alpha_i b - c_f}{c_v} \\end{equation*}\\] where \\(\\alpha_i\\) is the proportion of the total budget devoted to area \\(i\\), \\(c_f\\) is the fixed cost, and \\(c_v\\) is the variable cost. Note that \\(\\sum_{i=1}^{4}\\alpha_i = 1\\), and \\(\\alpha_2 = 0\\). Chauvenet et al. built a sophisticated dynamic optimization model to get the exact answer, but I’ll just evaluate some obvious alternatives here (Table 8.1). Figure 8.5: Extinction probability as a function of budget for areas 4 (solid line) and 1 (dashed line). Parameters given in Table 8.2. The next step in the process is to calculate the consequences of different alternatives. This requires us to connect the effort in an area to a reduction in the extinction probability. Chauvenet et al. choose a simple model \\[\\begin{equation} Pr\\left\\{\\text{area i extinct}|\\alpha_i b\\right\\} = \\frac{P_{0i}} {\\phi_i \\frac{\\alpha_i b - c_f}{c_v}+1} \\end{equation}\\] where \\(\\phi_i\\) is a coefficient that describes how effective anti-poaching effort is at reducing the extinction risk in area \\(i\\). Chauvenet et al. estimated these coefficients for each area from the results presented in Linkie et al. When \\(\\alpha_i = 0\\) then the extinction probability for area \\(i\\) is just \\(P_{0i}\\). As poaching effort increases, the extinction probability decreases hyperbolically towards zero. This means that initially small levels of effort make a big difference, but the gain per unit effort decreases as effort increases. This function gives me the actual probability of extinction for each area, which I then use in (8.3) to calculate the probability that one or more areas go extinct over 50 years (Table 8.3). Table 8.2: Parameters of the model from Chauvenet et al 2010. All costs in US$. Parameter Symbol Value Unmanaged probability of extinction \\(P_{0i}\\) 1 0.088 2 0.000 3 0.088 4 0.205 Management efficiency \\(\\phi_i\\) 1 0.046 2 0.000 3 0.016 4 0.060 Total budget \\(b\\) 52704.000 Fixed costs of managing subpopulation \\(c_f\\) 1728.000 Variable cost of management action (per unit effort) \\(c_v\\) 220.000 Splitting the budget equally or according to the extinction risk in each area give nearly identical results, about 3 times better than doing nothing. These rules are very close to the performance obtained with the full dynamic optimization model. Putting all the effort into the most at risk population gives a substantially worse performance. Even so, with the particular budget available the expected time to the first extinction is still only about 10 years for the best options (1/Pr(1 or more extinct)). There is only one objective so there is no need to consider tradeoffs among competing objectives. Table 8.3: Consequences of alternative budget allocation rules. Alternative Values Do Nothing 0.34 Split budget equally 0.097 Split budget in proportion to extinction risk 0.099 Put the whole budget in 4 0.18 References "],
["references.html", "References", " References "]
]
